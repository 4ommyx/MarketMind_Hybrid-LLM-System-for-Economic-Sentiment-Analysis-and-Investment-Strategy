{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a05c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can use the GPU.\n",
      "Number of available GPUs: 1\n",
      "Current GPU Name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use the GPU.\")\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19757519",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df346815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Loaded 10816 existing articles from csv_checkpoint/investing_news_realtime.csv\n",
      "\n",
      "[Page 1] Scanning for new links -> https://www.investing.com/news/stock-market-news/1\n",
      "Info: Found 35 links on page 1\n",
      "Stop Signal: Found existing article 'Investing.com‚Äôs stocks of the ...'. Stopping.\n",
      "\n",
      "================================================================================\n",
      "No new articles found. The file is up to date.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from urllib.parse import urljoin, urlsplit, urlunsplit\n",
    "\n",
    "# Third-party imports\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "BASE_URL = \"https://www.investing.com/news/stock-market-news\"\n",
    "DOMAIN = \"https://www.investing.com\"\n",
    "BROWSER_CONFIG = {\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True}\n",
    "OUTPUT_FILENAME = \"csv_checkpoint/investing_news_realtime.csv\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "def normalize_link(url):\n",
    "    \"\"\"\n",
    "    Normalizes a URL by removing query parameters and fragments.\n",
    "    \"\"\"\n",
    "    parts = urlsplit(url)\n",
    "    return urlunsplit((parts.scheme, parts.netloc, parts.path, \"\", \"\"))\n",
    "\n",
    "def extract_clean_text(raw_html):\n",
    "    \"\"\"\n",
    "    Parses HTML content, removes unnecessary tags (scripts, styles, etc.),\n",
    "    and extracts clean paragraph text from the article body.\n",
    "    \"\"\"\n",
    "    if not raw_html:\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    # Remove non-content tags\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"header\", \"footer\"]): \n",
    "        tag.decompose()\n",
    "\n",
    "    ignore_phrases = [\n",
    "        \"generated with the support of AI\", \n",
    "        \"reviewed by an editor\",\n",
    "        \"Join our investing challenges\", \n",
    "        \"InvestingPro\",\n",
    "        \"For more information see our T&C\", \n",
    "        \"Position:\"\n",
    "    ]\n",
    "\n",
    "    # Attempt to locate the main article body using common selectors\n",
    "    article_body = (\n",
    "        soup.find(\"div\", class_=\"WYSIWYG articlePage\") or\n",
    "        soup.find(\"div\", class_=\"article_container\") or\n",
    "        soup.find(\"div\", id=\"articleContent\") or\n",
    "        soup.find(\"div\", class_=\"article-content\") or\n",
    "        soup.body\n",
    "    )\n",
    "\n",
    "    paragraphs = []\n",
    "    if article_body:\n",
    "        for p in article_body.find_all(\"p\"):\n",
    "            text = p.get_text(\" \", strip=True)\n",
    "            # Filter out short texts or ignored phrases\n",
    "            if len(text) > 30 and not any(phrase in text for phrase in ignore_phrases):\n",
    "                paragraphs.append(text)\n",
    "\n",
    "    return \"\\n\\n\".join(paragraphs).strip()\n",
    "\n",
    "def load_existing_links(filename):\n",
    "    \"\"\"\n",
    "    Loads the existing CSV file and returns a set of links that have already been scraped.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return set()\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        if \"Link\" in df.columns:\n",
    "            # Normalize links in the file to ensure matching works correctly\n",
    "            return set(df[\"Link\"].apply(normalize_link).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read existing file {filename}: {e}\")\n",
    "    \n",
    "    return set()\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN SCRAPER FUNCTION\n",
    "# ==========================================\n",
    "def run_incremental_scraper(max_pages=50):\n",
    "    \"\"\"\n",
    "    Scrapes news articles starting from page 1.\n",
    "    Stops automatically when it encounters an article that is already in the CSV file.\n",
    "    \"\"\"\n",
    "    scraper = cloudscraper.create_scraper(browser=BROWSER_CONFIG)\n",
    "    scraper.headers.update({\"Accept-Language\": \"en-US,en;q=0.9\"})\n",
    "\n",
    "    # 1. Load existing data to check for duplicates\n",
    "    existing_links = load_existing_links(OUTPUT_FILENAME)\n",
    "    print(f\"Status: Loaded {len(existing_links)} existing articles from {OUTPUT_FILENAME}\")\n",
    "\n",
    "    new_articles = []\n",
    "    seen_links_session = set()\n",
    "    stop_scraping = False\n",
    "    \n",
    "    # Loop through pages (limited by max_pages to prevent infinite loops if something goes wrong)\n",
    "    for page in range(1, max_pages + 1):\n",
    "        if stop_scraping:\n",
    "            break\n",
    "\n",
    "        current_url = f\"{BASE_URL}/{page}\"\n",
    "        print(f\"\\n[Page {page}] Scanning for new links -> {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = scraper.get(current_url, timeout=20)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error: Could not access page {page} (Status: {response.status_code})\")\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            title_links = soup.find_all(\"a\", attrs={\"data-test\": \"article-title-link\"})\n",
    "            \n",
    "            if not title_links:\n",
    "                print(\"Info: No articles found on this page. Ending scrape.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Info: Found {len(title_links)} links on page {page}\")\n",
    "\n",
    "            current_page_candidates = []\n",
    "\n",
    "            # --- STEP 1: Filter Links ---\n",
    "            for a_tag in title_links:\n",
    "                href = a_tag.get(\"href\")\n",
    "                if not href: continue\n",
    "                \n",
    "                full_link = normalize_link(href if href.startswith(\"http\") else urljoin(DOMAIN, href))\n",
    "                \n",
    "                # CHECK: If we find a link that is already in our file, we have reached old news.\n",
    "                if full_link in existing_links:\n",
    "                    print(f\"Stop Signal: Found existing article '{a_tag.get_text(strip=True)[:30]}...'. Stopping.\")\n",
    "                    stop_scraping = True\n",
    "                    break # Break the link loop\n",
    "                \n",
    "                # Check for session duplicates (e.g. pinned posts appearing on multiple pages)\n",
    "                if full_link in seen_links_session or \"comment\" in full_link:\n",
    "                    continue\n",
    "\n",
    "                # Prepare item for scraping\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                \n",
    "                # Extract metadata\n",
    "                container = (\n",
    "                    a_tag.find_parent(\"article\") or \n",
    "                    a_tag.find_parent(\"li\") or \n",
    "                    a_tag.find_parent(\"div\", class_=lambda x: x and \"article\" in x)\n",
    "                )\n",
    "                \n",
    "                date_val, source_name = \"Unknown\", \"Unknown\"\n",
    "                if container:\n",
    "                    t = container.find(\"time\", attrs={\"data-test\": \"article-publish-date\"})\n",
    "                    s = container.find(\"span\", attrs={\"data-test\": \"news-provider-name\"})\n",
    "                    if t: date_val = t.get(\"datetime\") or t.get_text(strip=True)\n",
    "                    if s: source_name = s.get_text(strip=True)\n",
    "\n",
    "                item = {\n",
    "                    \"Page\": page,\n",
    "                    \"Date\": date_val,\n",
    "                    \"Source\": source_name,\n",
    "                    \"Title\": title,\n",
    "                    \"Link\": full_link\n",
    "                }\n",
    "                current_page_candidates.append(item)\n",
    "                seen_links_session.add(full_link)\n",
    "\n",
    "            # --- STEP 2: Scrape Content for New Links ---\n",
    "            if current_page_candidates:\n",
    "                print(f\"Status: Found {len(current_page_candidates)} NEW articles on page {page}. Extracting content...\")\n",
    "                \n",
    "                for i, item in enumerate(current_page_candidates, start=1):\n",
    "                    print(f\"    [{i}/{len(current_page_candidates)}] Fetching: {item['Title'][:50]}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        scraper.headers.update({\"Referer\": BASE_URL})\n",
    "                        resp = scraper.get(item[\"Link\"], timeout=20)\n",
    "                        content = extract_clean_text(resp.text)\n",
    "                        \n",
    "                        if content and len(content) > 100:\n",
    "                            item[\"Content\"] = content\n",
    "                            new_articles.append(item)\n",
    "                        else:\n",
    "                            print(f\"        Warning: Content too short/empty.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"        Error fetching article: {e}\")\n",
    "                    \n",
    "                    time.sleep(random.uniform(1.5, 3))\n",
    "\n",
    "            else:\n",
    "                if not stop_scraping:\n",
    "                    print(\"Info: No valid new links found on this page (might be duplicates or ads).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Critical Error processing page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Delay between pages\n",
    "        if not stop_scraping:\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    # --- STEP 3: Save New Data ---\n",
    "    if new_articles:\n",
    "        df_new = pd.DataFrame(new_articles)\n",
    "        \n",
    "        # Check if file exists to determine if we need header\n",
    "        file_exists = os.path.isfile(OUTPUT_FILENAME)\n",
    "        \n",
    "        # Append to CSV\n",
    "        df_new.to_csv(OUTPUT_FILENAME, mode='a', header=not file_exists, index=False, encoding='utf-8-sig')\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"SUCCESS: Appended {len(df_new)} new articles to {OUTPUT_FILENAME}\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"No new articles found. The file is up to date.\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    return pd.DataFrame(new_articles)\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the incremental scraper\n",
    "    # It will stop automatically when it hits news that is already in the CSV\n",
    "    run_incremental_scraper(max_pages=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "042a5951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10816, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:56:10</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>BofA unveils its top 10 U.S. ideas for Q1 2026</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Bank of America has released ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:55:45</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com --¬†Canaccord Genuity analyst Geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:05:02</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Is Reddit the new homepage for the Open Web?</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Reddit is increasingly positi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 03:24:29</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Trump blocks chips deal, cites security, China...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>WASHINGTON, Jan 2 (Reuters) - President Donald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 01:12:24</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Top hedge funds led by D.E.Shaw, Bridgewater a...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>(Corrects Point72‚Äôs return figures in second b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 00:25:34</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>US FTC wins ruling blocking Edwards Lifescienc...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Jan 9 (Reuters) - The U.S. Federal Trade Commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10812</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 09:00:05</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Investing.com‚Äôs stocks of the week</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com ‚Äì With the first week of full tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10813</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 09:00:03</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Wolfe analysts say these will be key investmen...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Wolfe Research expects U.S. e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10814</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 08:30:03</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Will SMidCaps outperform in 2026?</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Small- and mid-cap (SMidCaps)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10815</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 08:30:02</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>BofA lists potential 2026 market-moving AI ann...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Artificial intelligence remai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10816 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page                 Date         Source  \\\n",
       "0         1  2026-01-03 09:56:10  Investing.com   \n",
       "1         1  2026-01-03 09:55:45  Investing.com   \n",
       "2         1  2026-01-03 09:05:02  Investing.com   \n",
       "3         1  2026-01-03 03:24:29        Reuters   \n",
       "4         1  2026-01-03 01:12:24        Reuters   \n",
       "...     ...                  ...            ...   \n",
       "10811     1  2026-01-10 00:25:34        Reuters   \n",
       "10812     1  2026-01-10 09:00:05  Investing.com   \n",
       "10813     1  2026-01-10 09:00:03  Investing.com   \n",
       "10814     1  2026-01-10 08:30:03  Investing.com   \n",
       "10815     1  2026-01-10 08:30:02  Investing.com   \n",
       "\n",
       "                                                   Title  \\\n",
       "0         BofA unveils its top 10 U.S. ideas for Q1 2026   \n",
       "1      Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...   \n",
       "2           Is Reddit the new homepage for the Open Web?   \n",
       "3      Trump blocks chips deal, cites security, China...   \n",
       "4      Top hedge funds led by D.E.Shaw, Bridgewater a...   \n",
       "...                                                  ...   \n",
       "10811  US FTC wins ruling blocking Edwards Lifescienc...   \n",
       "10812                 Investing.com‚Äôs stocks of the week   \n",
       "10813  Wolfe analysts say these will be key investmen...   \n",
       "10814                  Will SMidCaps outperform in 2026?   \n",
       "10815  BofA lists potential 2026 market-moving AI ann...   \n",
       "\n",
       "                                                    Link  \\\n",
       "0      https://www.investing.com/news/stock-market-ne...   \n",
       "1      https://www.investing.com/news/stock-market-ne...   \n",
       "2      https://www.investing.com/news/stock-market-ne...   \n",
       "3      https://www.investing.com/news/stock-market-ne...   \n",
       "4      https://www.investing.com/news/stock-market-ne...   \n",
       "...                                                  ...   \n",
       "10811  https://www.investing.com/news/stock-market-ne...   \n",
       "10812  https://www.investing.com/news/stock-market-ne...   \n",
       "10813  https://www.investing.com/news/stock-market-ne...   \n",
       "10814  https://www.investing.com/news/stock-market-ne...   \n",
       "10815  https://www.investing.com/news/stock-market-ne...   \n",
       "\n",
       "                                                 Content  \n",
       "0      Investing.com -- Bank of America has released ...  \n",
       "1      Investing.com --¬†Canaccord Genuity analyst Geo...  \n",
       "2      Investing.com -- Reddit is increasingly positi...  \n",
       "3      WASHINGTON, Jan 2 (Reuters) - President Donald...  \n",
       "4      (Corrects Point72‚Äôs return figures in second b...  \n",
       "...                                                  ...  \n",
       "10811  Jan 9 (Reuters) - The U.S. Federal Trade Commi...  \n",
       "10812  Investing.com ‚Äì With the first week of full tr...  \n",
       "10813  Investing.com -- Wolfe Research expects U.S. e...  \n",
       "10814  Investing.com -- Small- and mid-cap (SMidCaps)...  \n",
       "10815  Investing.com -- Artificial intelligence remai...  \n",
       "\n",
       "[10816 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv \n",
    "import pandas as pd\n",
    "df = pd.read_csv('csv_checkpoint/investing_news_realtime.csv')\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f740fa5",
   "metadata": {},
   "source": [
    "## IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b2eca",
   "metadata": {},
   "source": [
    "### IDX (TF-IDF Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0078e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ Vectorizing text and calculating similarity...\n",
      "üîç Analyzing sectors for each article...\n",
      "\n",
      "==================================================\n",
      "‚úÖ Processing Complete. Total rows: 10816\n",
      "==================================================\n",
      "                  Date                  Sector  Confidence  \\\n",
      "0  2026-01-03 09:56:10              Financials    0.044771   \n",
      "1  2026-01-03 09:55:45       Consumer Cyclical    0.049162   \n",
      "2  2026-01-03 09:05:02              Technology    0.032434   \n",
      "3  2026-01-03 03:24:29             Industrials    0.024838   \n",
      "4  2026-01-03 01:12:24              Financials    0.146302   \n",
      "5  2026-01-02 23:25:27  Communication Services    0.159893   \n",
      "6  2026-01-02 22:13:29                   Other    0.014220   \n",
      "7  2026-01-02 21:48:35               Utilities    0.134569   \n",
      "8  2026-01-02 21:43:09                   Other    0.019699   \n",
      "9  2026-01-02 21:31:23         Basic Materials    0.025417   \n",
      "\n",
      "                                         Sector_Dict  Sector_Count  \\\n",
      "0  {'Financials': 0.04477, 'Healthcare': 0.02542,...             3   \n",
      "1  {'Consumer Cyclical': 0.04916, 'Energy': 0.02481}             2   \n",
      "2                            {'Technology': 0.03243}             1   \n",
      "3                           {'Industrials': 0.02484}             1   \n",
      "4       {'Financials': 0.1463, 'Utilities': 0.03505}             2   \n",
      "5  {'Communication Services': 0.15989, 'Technolog...             2   \n",
      "6                                     {'Other': 0.0}             0   \n",
      "7  {'Utilities': 0.13457, 'Energy': 0.06653, 'Fin...             3   \n",
      "8                                     {'Other': 0.0}             0   \n",
      "9                       {'Basic Materials': 0.02542}             1   \n",
      "\n",
      "                                               Title  \n",
      "0     BofA unveils its top 10 U.S. ideas for Q1 2026  \n",
      "1  Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...  \n",
      "2       Is Reddit the new homepage for the Open Web?  \n",
      "3  Trump blocks chips deal, cites security, China...  \n",
      "4  Top hedge funds led by D.E.Shaw, Bridgewater a...  \n",
      "5  Short interest in Trump Media climbs after rec...  \n",
      "6  Trump blocks HieFo‚Äôs acquisition of Emcore sem...  \n",
      "7  NextEra Energy stock rises after confirming fi...  \n",
      "8  BigBear.ai stock falls after executives disclo...  \n",
      "9  U.S. stocks mixed at close of trade; Dow Jones...  \n",
      "\n",
      "üìä Sector Distribution (Primary):\n",
      "Sector\n",
      "Financials                2212\n",
      "Other                     1998\n",
      "Technology                1391\n",
      "Healthcare                 970\n",
      "Consumer Cyclical          865\n",
      "Energy                     800\n",
      "Industrials                653\n",
      "Basic Materials            574\n",
      "Communication Services     536\n",
      "Utilities                  445\n",
      "Consumer Defensive         283\n",
      "Real Estate                 89\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "CSV_PATH = 'csv_checkpoint/investing_news_realtime.csv'\n",
    "THRESHOLD = 0.02\n",
    "MAX_LABELS = 3\n",
    "\n",
    "SECTOR_KEYWORDS = {\n",
    "    \"Technology\": (\n",
    "        \"technology software semiconductor chip artificial intelligence ai cloud computing \"\n",
    "        \"cybersecurity hardware electronics data center server processor gpu cpu saas \"\n",
    "        \"it services digital platform quantum computing machine learning automation \"\n",
    "        \"network infrastructure operating system application developer tech\"\n",
    "    ),\n",
    "    \"Communication Services\": (\n",
    "        \"communication internet telecommunication telecom media entertainment streaming \"\n",
    "        \"social media advertising broadcasting broadband wireless network cable satellite \"\n",
    "        \"interactive media publishing movies gaming video content provider\"\n",
    "    ),\n",
    "    \"Consumer Cyclical\": (\n",
    "        \"consumer discretionary retail e-commerce automotive vehicle electric vehicle ev \"\n",
    "        \"car auto parts restaurant travel leisure hotel resort casino gambling apparel \"\n",
    "        \"luxury goods home improvement department store textile footwear consumer services\"\n",
    "    ),\n",
    "    \"Financials\": (\n",
    "        \"financial banking bank investment asset management insurance credit fintech \"\n",
    "        \"capital markets wealth management interest rate monetary policy federal reserve \"\n",
    "        \"fed loan mortgage equity trading brokerage payment system currency exchange \"\n",
    "        \"private equity hedge fund venture capital audit tax\"\n",
    "    ),\n",
    "    \"Healthcare\": (\n",
    "        \"healthcare health pharmaceutical biotech biotechnology medical device \"\n",
    "        \"drug vaccine clinical trial fda approval hospital health insurance \"\n",
    "        \"life sciences diagnosis therapy treatment genomics medical equipment \"\n",
    "        \"managed care pharmacy research development r&d\"\n",
    "    ),\n",
    "    \"Energy\": (\n",
    "        \"energy oil gas petroleum crude drilling exploration production pipeline \"\n",
    "        \"refining refinery renewable energy solar wind biofuel carbon capture \"\n",
    "        \"energy equipment services natural gas lng offshore onshore fuel power generation\"\n",
    "    ),\n",
    "    \"Industrials\": (\n",
    "        \"industrial aerospace defense machinery transportation logistics airline \"\n",
    "        \"freight railroad shipping trucking manufacturing construction engineering \"\n",
    "        \"building products electrical equipment commercial services waste management \"\n",
    "        \"infrastructure conglomerate supply chain\"\n",
    "    ),\n",
    "    \"Consumer Defensive\": (\n",
    "        \"consumer staples food beverage household products personal care tobacco \"\n",
    "        \"supermarket grocery hypermarket discount store agriculture products \"\n",
    "        \"packaged food hygiene cleaning products soft drink alcohol brewing\"\n",
    "    ),\n",
    "    \"Real Estate\": (\n",
    "        \"real estate reit property housing residential commercial industrial \"\n",
    "        \"leasing tenant development management brokerage mortgage reit \"\n",
    "        \"data center reit tower reit healthcare reit hotel reit office reit retail reit\"\n",
    "    ),\n",
    "    \"Utilities\": (\n",
    "        \"utilities electric power water gas utility renewable utility grid \"\n",
    "        \"transmission distribution energy infrastructure clean energy nuclear \"\n",
    "        \"independent power producer multi-utilities\"\n",
    "    ),\n",
    "    \"Basic Materials\": (\n",
    "        \"basic materials chemicals mining metals steel gold copper silver \"\n",
    "        \"agriculture fertilizer construction materials packaging container \"\n",
    "        \"paper forest products specialty chemicals industrial gases commodity \"\n",
    "        \"aluminum iron ore lithium rare earth\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. CLASSIFIER CLASS\n",
    "# ==========================================\n",
    "class SectorClassifier:\n",
    "    def __init__(self, keywords: Dict[str, str]):\n",
    "        self.sector_names = list(keywords.keys())\n",
    "        self.sector_docs = list(keywords.values())\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    def classify(self, df: pd.DataFrame, text_col: str, threshold: float = 0.02, max_labels: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs TF-IDF vectorization and cosine similarity to assign sectors.\n",
    "        \"\"\"\n",
    "        print(\"üßÆ Vectorizing text and calculating similarity...\")\n",
    "        \n",
    "        # Prepare Corpus: Combine Sector Keywords + News Content\n",
    "        all_docs = self.sector_docs + df[text_col].tolist()\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(all_docs)\n",
    "\n",
    "        # Separate matrices\n",
    "        sector_vectors = tfidf_matrix[:len(self.sector_names)]\n",
    "        news_vectors = tfidf_matrix[len(self.sector_names):]\n",
    "\n",
    "        # Calculate Similarity\n",
    "        similarity_scores = cosine_similarity(news_vectors, sector_vectors)\n",
    "\n",
    "        # Prepare result containers\n",
    "        primary_sectors = []\n",
    "        confidences = []\n",
    "        sector_dicts = []\n",
    "        sector_counts = []\n",
    "\n",
    "        print(\"üîç Analyzing sectors for each article...\")\n",
    "        \n",
    "        for scores in similarity_scores:\n",
    "            # --- Logic Part 1: Single Best Sector (Original Logic) ---\n",
    "            best_idx = scores.argmax()\n",
    "            max_score = scores.max()\n",
    "            \n",
    "            if max_score > threshold:\n",
    "                primary_sectors.append(self.sector_names[best_idx])\n",
    "            else:\n",
    "                primary_sectors.append(\"Other\")\n",
    "            \n",
    "            confidences.append(max_score)\n",
    "\n",
    "            # --- Logic Part 2: Multi-Label Top N (Refined Logic) ---\n",
    "            # 1. Filter by threshold\n",
    "            qualified_indices = np.where(scores > threshold)[0]\n",
    "\n",
    "            if len(qualified_indices) == 0:\n",
    "                sector_dicts.append({'Other': 0.0})\n",
    "                sector_counts.append(0)\n",
    "            else:\n",
    "                # 2. Sort by score descending\n",
    "                qualified_scores = scores[qualified_indices]\n",
    "                # argsort gives ascending, so we reverse it [::-1]\n",
    "                sorted_indices_local = np.argsort(qualified_scores)[::-1]\n",
    "\n",
    "                # 3. Take Top N\n",
    "                top_indices_local = sorted_indices_local[:max_labels]\n",
    "                final_indices = qualified_indices[top_indices_local]\n",
    "\n",
    "                # 4. Create Dictionary\n",
    "                current_dict = {\n",
    "                    self.sector_names[i]: round(float(scores[i]), 5)\n",
    "                    for i in final_indices\n",
    "                }\n",
    "                sector_dicts.append(current_dict)\n",
    "                sector_counts.append(len(current_dict))\n",
    "\n",
    "        # Assign back to DataFrame\n",
    "        df['Sector'] = primary_sectors\n",
    "        df['Confidence'] = confidences\n",
    "        df['Sector_Dict'] = sector_dicts\n",
    "        df['Sector_Count'] = sector_counts\n",
    "\n",
    "        return df\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN EXECUTION\n",
    "# ==========================================\n",
    "def load_and_prep_data(filepath: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Combine Title and Content, fill NaNs\n",
    "        df['full_text'] = df['Title'].fillna('') + \" \" + df['Content'].fillna('')\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File not found at {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    df_news = load_and_prep_data(CSV_PATH)\n",
    "\n",
    "    if not df_news.empty:\n",
    "        # 2. Initialize Classifier\n",
    "        classifier = SectorClassifier(SECTOR_KEYWORDS)\n",
    "\n",
    "        # 3. Process Data\n",
    "        df_result = classifier.classify(\n",
    "            df_news, \n",
    "            text_col='full_text', \n",
    "            threshold=THRESHOLD, \n",
    "            max_labels=MAX_LABELS\n",
    "        )\n",
    "\n",
    "        # 4. Display Results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"‚úÖ Processing Complete. Total rows: {len(df_result)}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show Sample\n",
    "        cols_to_show = ['Date', 'Sector', 'Confidence', 'Sector_Dict', 'Sector_Count', 'Title']\n",
    "        print(df_result[cols_to_show].head(10))\n",
    "        \n",
    "        print(\"\\nüìä Sector Distribution (Primary):\")\n",
    "        print(df_result['Sector'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0035f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop fulltext and save csv file\n",
    "df_result\n",
    "df_result_drop = df_result.drop(\"full_text\", axis=1)\n",
    "df_result_drop = df_result_drop.to_csv(\"csv_checkpoint/investing_news_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ea658b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Sector_Dict</th>\n",
       "      <th>Sector_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:56:10</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>BofA unveils its top 10 U.S. ideas for Q1 2026</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Bank of America has released ...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>0.044771</td>\n",
       "      <td>{'Financials': 0.04477, 'Healthcare': 0.02542,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:55:45</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com --¬†Canaccord Genuity analyst Geo...</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>0.049162</td>\n",
       "      <td>{'Consumer Cyclical': 0.04916, 'Energy': 0.02481}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:05:02</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Is Reddit the new homepage for the Open Web?</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Reddit is increasingly positi...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.032434</td>\n",
       "      <td>{'Technology': 0.03243}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 03:24:29</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Trump blocks chips deal, cites security, China...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>WASHINGTON, Jan 2 (Reuters) - President Donald...</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>0.024838</td>\n",
       "      <td>{'Industrials': 0.02484}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 01:12:24</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Top hedge funds led by D.E.Shaw, Bridgewater a...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>(Corrects Point72‚Äôs return figures in second b...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>0.146302</td>\n",
       "      <td>{'Financials': 0.1463, 'Utilities': 0.03505}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 00:25:34</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>US FTC wins ruling blocking Edwards Lifescienc...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Jan 9 (Reuters) - The U.S. Federal Trade Commi...</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0.046365</td>\n",
       "      <td>{'Healthcare': 0.04636}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10812</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 09:00:05</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Investing.com‚Äôs stocks of the week</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com ‚Äì With the first week of full tr...</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>0.098161</td>\n",
       "      <td>{'Utilities': 0.09816, 'Energy': 0.05548, 'Tec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10813</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 09:00:03</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Wolfe analysts say these will be key investmen...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Wolfe Research expects U.S. e...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>0.093048</td>\n",
       "      <td>{'Financials': 0.09305, 'Consumer Cyclical': 0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10814</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 08:30:03</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Will SMidCaps outperform in 2026?</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Small- and mid-cap (SMidCaps)...</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>0.024086</td>\n",
       "      <td>{'Healthcare': 0.02409}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10815</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-10 08:30:02</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>BofA lists potential 2026 market-moving AI ann...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Artificial intelligence remai...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.059103</td>\n",
       "      <td>{'Technology': 0.0591, 'Consumer Cyclical': 0....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10816 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page                 Date         Source  \\\n",
       "0         1  2026-01-03 09:56:10  Investing.com   \n",
       "1         1  2026-01-03 09:55:45  Investing.com   \n",
       "2         1  2026-01-03 09:05:02  Investing.com   \n",
       "3         1  2026-01-03 03:24:29        Reuters   \n",
       "4         1  2026-01-03 01:12:24        Reuters   \n",
       "...     ...                  ...            ...   \n",
       "10811     1  2026-01-10 00:25:34        Reuters   \n",
       "10812     1  2026-01-10 09:00:05  Investing.com   \n",
       "10813     1  2026-01-10 09:00:03  Investing.com   \n",
       "10814     1  2026-01-10 08:30:03  Investing.com   \n",
       "10815     1  2026-01-10 08:30:02  Investing.com   \n",
       "\n",
       "                                                   Title  \\\n",
       "0         BofA unveils its top 10 U.S. ideas for Q1 2026   \n",
       "1      Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...   \n",
       "2           Is Reddit the new homepage for the Open Web?   \n",
       "3      Trump blocks chips deal, cites security, China...   \n",
       "4      Top hedge funds led by D.E.Shaw, Bridgewater a...   \n",
       "...                                                  ...   \n",
       "10811  US FTC wins ruling blocking Edwards Lifescienc...   \n",
       "10812                 Investing.com‚Äôs stocks of the week   \n",
       "10813  Wolfe analysts say these will be key investmen...   \n",
       "10814                  Will SMidCaps outperform in 2026?   \n",
       "10815  BofA lists potential 2026 market-moving AI ann...   \n",
       "\n",
       "                                                    Link  \\\n",
       "0      https://www.investing.com/news/stock-market-ne...   \n",
       "1      https://www.investing.com/news/stock-market-ne...   \n",
       "2      https://www.investing.com/news/stock-market-ne...   \n",
       "3      https://www.investing.com/news/stock-market-ne...   \n",
       "4      https://www.investing.com/news/stock-market-ne...   \n",
       "...                                                  ...   \n",
       "10811  https://www.investing.com/news/stock-market-ne...   \n",
       "10812  https://www.investing.com/news/stock-market-ne...   \n",
       "10813  https://www.investing.com/news/stock-market-ne...   \n",
       "10814  https://www.investing.com/news/stock-market-ne...   \n",
       "10815  https://www.investing.com/news/stock-market-ne...   \n",
       "\n",
       "                                                 Content             Sector  \\\n",
       "0      Investing.com -- Bank of America has released ...         Financials   \n",
       "1      Investing.com --¬†Canaccord Genuity analyst Geo...  Consumer Cyclical   \n",
       "2      Investing.com -- Reddit is increasingly positi...         Technology   \n",
       "3      WASHINGTON, Jan 2 (Reuters) - President Donald...        Industrials   \n",
       "4      (Corrects Point72‚Äôs return figures in second b...         Financials   \n",
       "...                                                  ...                ...   \n",
       "10811  Jan 9 (Reuters) - The U.S. Federal Trade Commi...         Healthcare   \n",
       "10812  Investing.com ‚Äì With the first week of full tr...          Utilities   \n",
       "10813  Investing.com -- Wolfe Research expects U.S. e...         Financials   \n",
       "10814  Investing.com -- Small- and mid-cap (SMidCaps)...         Healthcare   \n",
       "10815  Investing.com -- Artificial intelligence remai...         Technology   \n",
       "\n",
       "       Confidence                                        Sector_Dict  \\\n",
       "0        0.044771  {'Financials': 0.04477, 'Healthcare': 0.02542,...   \n",
       "1        0.049162  {'Consumer Cyclical': 0.04916, 'Energy': 0.02481}   \n",
       "2        0.032434                            {'Technology': 0.03243}   \n",
       "3        0.024838                           {'Industrials': 0.02484}   \n",
       "4        0.146302       {'Financials': 0.1463, 'Utilities': 0.03505}   \n",
       "...           ...                                                ...   \n",
       "10811    0.046365                            {'Healthcare': 0.04636}   \n",
       "10812    0.098161  {'Utilities': 0.09816, 'Energy': 0.05548, 'Tec...   \n",
       "10813    0.093048  {'Financials': 0.09305, 'Consumer Cyclical': 0...   \n",
       "10814    0.024086                            {'Healthcare': 0.02409}   \n",
       "10815    0.059103  {'Technology': 0.0591, 'Consumer Cyclical': 0....   \n",
       "\n",
       "       Sector_Count  \n",
       "0                 3  \n",
       "1                 2  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 2  \n",
       "...             ...  \n",
       "10811             1  \n",
       "10812             3  \n",
       "10813             3  \n",
       "10814             1  \n",
       "10815             3  \n",
       "\n",
       "[10816 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read_csv \n",
    "df = pd.read_csv(\"csv_checkpoint/investing_news_tfidf.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3266f",
   "metadata": {},
   "source": [
    "### IDX(LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceecb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "# MY_HUGGIEFACE_TOKEN = \"\" # USE you OWN HUGGIING FACE TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccfe79ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Rows to classify by AI: 1998\n",
      "üöÄ [Step 1] Loading AI Model: Qwen/Qwen2.5-14B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 378/579 [00:27<00:14, 13.76it/s, Materializing param=model.layers.31.mlp.up_proj.weight]             \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 8.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# 4. MAIN PIPELINE\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    177\u001b[39m     \u001b[38;5;66;03m# 1. Run AI Process\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     success = \u001b[43mrun_llm_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# 2. Run Merge Process\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mrun_llm_process\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Rows to classify by AI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(target_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target_indices) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     classifier = \u001b[43mNewsClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(target_indices), Config.BATCH_SIZE), desc=\u001b[33m\"\u001b[39m\u001b[33mü§ñ AI Processing\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mNewsClassifier.__init__\u001b[39m\u001b[34m(self, model_name, device)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.tokenizer.pad_token = \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.device = device\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4076\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4073\u001b[39m     device_map = _get_device_map(model, device_map, max_memory, hf_quantizer)\n\u001b[32m   4075\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4076\u001b[39m model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4082\u001b[39m \u001b[43m    \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4085\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_conversions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4091\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4093\u001b[39m model.eval()  \u001b[38;5;66;03m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[39;00m\n\u001b[32m   4094\u001b[39m model.set_use_kernels(use_kernels, kernel_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4220\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_buffers, dtype, hf_quantizer, device_mesh, weights_only, weight_mapping)\u001b[39m\n\u001b[32m   4216\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNeither a state dict nor checkpoint files were found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4219\u001b[39m missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors = (\n\u001b[32m-> \u001b[39m\u001b[32m4220\u001b[39m     \u001b[43mconvert_and_load_state_dict_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtp_plan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tp_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_plan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4232\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4233\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4234\u001b[39m )\n\u001b[32m   4236\u001b[39m \u001b[38;5;66;03m# finally close all opened file pointers\u001b[39;00m\n\u001b[32m   4237\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_pointer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py:1100\u001b[39m, in \u001b[36mconvert_and_load_state_dict_in_model\u001b[39m\u001b[34m(model, state_dict, weight_mapping, tp_plan, hf_quantizer, dtype, device_map, dtype_plan, device_mesh, disk_offload_index, disk_offload_folder, offload_buffers)\u001b[39m\n\u001b[32m   1098\u001b[39m pbar.refresh()\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     realized_value, conversion_errors = \u001b[43mmapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfirst_param_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconversion_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversion_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m target_name, param \u001b[38;5;129;01min\u001b[39;00m realized_value.items():\n\u001b[32m   1109\u001b[39m         param = param[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m param\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py:580\u001b[39m, in \u001b[36mWeightRenaming.convert\u001b[39m\u001b[34m(self, layer_name, model, config, hf_quantizer, missing_keys, conversion_errors)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    570\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    571\u001b[39m     layer_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    578\u001b[39m     \u001b[38;5;66;03m# Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\u001b[39;00m\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# attribute during the whole process\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     collected_tensors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaterialize_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m     \u001b[38;5;66;03m# Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\u001b[39;00m\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# only be of length 1, and are actually the full key names - we also have only 1 single related tensor)\u001b[39;00m\n\u001b[32m    584\u001b[39m     target_key = \u001b[38;5;28mself\u001b[39m.target_patterns[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py:555\u001b[39m, in \u001b[36mWeightTransform.materialize_tensors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# Async loading\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors[\u001b[32m0\u001b[39m], Future):\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m     tensors = [\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# Sync loading\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(tensors[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py:703\u001b[39m, in \u001b[36mspawn_materialize.<locals>._job\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_job\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_materialize_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-project/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py:692\u001b[39m, in \u001b[36m_materialize_copy\u001b[39m\u001b[34m(tensor, device, dtype)\u001b[39m\n\u001b[32m    690\u001b[39m tensor = tensor[...]\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 8.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import ast\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    # Files\n",
    "    TFIDF_FILE = 'csv_checkpoint/investing_news_tfidf.csv'   # Input 1: ‡∏ú‡∏•‡∏à‡∏≤‡∏Å TF-IDF\n",
    "    LLM_TEMP_FILE = 'csv_checkpoint/investing_news_llm.csv'  # Temp Output: ‡∏ú‡∏•‡∏à‡∏≤‡∏Å AI (Save ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏≤‡∏á)\n",
    "    FINAL_OUTPUT_FILE = 'df_final_result.csv'                # Final Output: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "    \n",
    "    # Model Settings\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "    BATCH_SIZE = 16\n",
    "    DEVICE = \"cuda:0\"\n",
    "    \n",
    "    EXISTING_SECTORS = [\n",
    "        'Financials', 'Technology', 'Healthcare', 'Consumer Cyclical',\n",
    "        'Energy', 'Industrials', 'Basic Materials', 'Communication Services',\n",
    "        'Utilities', 'Consumer Defensive', 'Real Estate'\n",
    "    ]\n",
    "\n",
    "# ==========================================\n",
    "# 2. STEP 1: LLM CLASSIFIER (AI Logic)\n",
    "# ==========================================\n",
    "def sanitize_sector_output(sector: Any) -> str:\n",
    "    if isinstance(sector, list): return \",\".join([str(s) for s in sector])\n",
    "    elif isinstance(sector, dict): return str(sector)\n",
    "    return str(sector)\n",
    "\n",
    "def parse_llm_response(response: str) -> str:\n",
    "    try:\n",
    "        clean_json = response.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        start = clean_json.find('{')\n",
    "        end = clean_json.rfind('}') + 1\n",
    "        if start != -1 and end != -1:\n",
    "            data = json.loads(clean_json[start:end])\n",
    "            return data.get(\"sector\", \"Other\")\n",
    "        return \"Other\"\n",
    "    except: return \"Other\"\n",
    "\n",
    "class NewsClassifier:\n",
    "    def __init__(self, model_name: str, device: str):\n",
    "        print(f\"üöÄ [Step 1] Loading AI Model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "        if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=device)\n",
    "        self.device = device\n",
    "\n",
    "    def batch_predict(self, titles: List[str], contents: List[str]) -> List[str]:\n",
    "        prompts = []\n",
    "        for t, c in zip(titles, contents):\n",
    "            text = f\"\"\"Classify into JSON.\n",
    "Sectors: {json.dumps(Config.EXISTING_SECTORS)}\n",
    "If unrelated, use \"Other\".\n",
    "News: \"{t}\"\n",
    "Snippet: \"{str(c)[:500]}...\"\n",
    "Format: {{\"sector\": \"...\"}}\"\"\"\n",
    "            messages = [{\"role\": \"user\", \"content\": text}]\n",
    "            prompts.append(self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "        \n",
    "        inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(**inputs, max_new_tokens=40, temperature=0.1, do_sample=False)\n",
    "        \n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        generated_ids = generated_ids[:, input_len:]\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    def free_memory(self):\n",
    "        print(\"üßπ [Cleanup] Clearing VRAM...\")\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"‚úÖ VRAM Cleared. Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "def run_llm_process():\n",
    "    if not os.path.exists(Config.TFIDF_FILE):\n",
    "        print(f\"‚ùå Error: Input file {Config.TFIDF_FILE} missing.\")\n",
    "        return False\n",
    "\n",
    "    df = pd.read_csv(Config.TFIDF_FILE)\n",
    "    if 'AI_Sector' not in df.columns: df['AI_Sector'] = None\n",
    "\n",
    "    # Filter only 'Other' or NaN\n",
    "    mask = (df['Sector'] == 'Other') | (df['Sector'].isna())\n",
    "    target_indices = df[mask].index.tolist()\n",
    "    print(f\"üìä Rows to classify by AI: {len(target_indices)}\")\n",
    "\n",
    "    if len(target_indices) > 0:\n",
    "        classifier = NewsClassifier(Config.MODEL_NAME, Config.DEVICE)\n",
    "        try:\n",
    "            for i in tqdm(range(0, len(target_indices), Config.BATCH_SIZE), desc=\"ü§ñ AI Processing\"):\n",
    "                batch_idx = target_indices[i : i + Config.BATCH_SIZE]\n",
    "                batch_titles = df.loc[batch_idx, 'Title'].tolist()\n",
    "                batch_contents = df.loc[batch_idx, 'Content'].tolist()\n",
    "                \n",
    "                raw_responses = classifier.batch_predict(batch_titles, batch_contents)\n",
    "                \n",
    "                for idx, resp in zip(batch_idx, raw_responses):\n",
    "                    clean_sector = sanitize_sector_output(parse_llm_response(resp))\n",
    "                    try: df.at[idx, 'AI_Sector'] = clean_sector\n",
    "                    except: df.loc[idx, 'AI_Sector'] = clean_sector\n",
    "\n",
    "                if (i // Config.BATCH_SIZE) % 5 == 0:\n",
    "                    df.to_csv(Config.LLM_TEMP_FILE, index=False)\n",
    "        finally:\n",
    "            classifier.free_memory() # üî• Clear VRAM immediately after loop\n",
    "\n",
    "    # Save final LLM result\n",
    "    df.to_csv(Config.LLM_TEMP_FILE, index=False)\n",
    "    print(f\"üíæ AI Results saved to {Config.LLM_TEMP_FILE}\")\n",
    "    return True\n",
    "\n",
    "# ==========================================\n",
    "# 3. STEP 2: MERGER & FINAL LOGIC\n",
    "# ==========================================\n",
    "class ResultMerger:\n",
    "    def _determine_sector(self, row):\n",
    "        # 1. Check TF-IDF result first\n",
    "        sector_dict_str = row.get('Sector_Dict', '{}')\n",
    "        sector_count = row.get('Sector_Count', 0)\n",
    "        \n",
    "        valid_keys = []\n",
    "        try:\n",
    "            val_dict = ast.literal_eval(sector_dict_str) if isinstance(sector_dict_str, str) else sector_dict_str\n",
    "            if isinstance(val_dict, dict):\n",
    "                valid_keys = list(val_dict.keys())\n",
    "                if len(valid_keys) > 1 and 'Other' in valid_keys:\n",
    "                    valid_keys.remove('Other')\n",
    "        except: pass\n",
    "\n",
    "        # Logic: If TF-IDF found valid sectors -> Use them. Else -> Use AI.\n",
    "        if sector_count > 0 and valid_keys != ['Other'] and valid_keys:\n",
    "            return \", \".join(valid_keys)\n",
    "        else:\n",
    "            ai_val = row.get('AI_Sector')\n",
    "            return str(ai_val) if pd.notna(ai_val) and str(ai_val).strip() != \"\" else \"Other\"\n",
    "\n",
    "    def process(self):\n",
    "        print(\"\\nüîó [Step 2] Merging & Finalizing Sectors...\")\n",
    "        \n",
    "        # Load & Merge\n",
    "        df_tfidf = pd.read_csv(Config.TFIDF_FILE)\n",
    "        try:\n",
    "            df_llm = pd.read_csv(Config.LLM_TEMP_FILE)\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ö†Ô∏è No LLM file found, using TF-IDF only.\")\n",
    "            df_llm = pd.DataFrame()\n",
    "\n",
    "        # Vertical Concat & Deduplicate (Prioritize LLM/Last file)\n",
    "        df_combined = pd.concat([df_tfidf, df_llm], ignore_index=True)\n",
    "        df_combined = df_combined.drop_duplicates(subset=['Link'], keep='last')\n",
    "        \n",
    "        # Apply Logic\n",
    "        df_combined['Combined_Sector'] = df_combined.apply(self._determine_sector, axis=1)\n",
    "        \n",
    "        # Save Final\n",
    "        df_combined.to_csv(Config.FINAL_OUTPUT_FILE, index=False)\n",
    "        print(f\"‚úÖ SUCCESS! Final data saved to: {Config.FINAL_OUTPUT_FILE}\")\n",
    "        print(f\"   Total Rows: {len(df_combined)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN PIPELINE\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Run AI Process\n",
    "    success = run_llm_process()\n",
    "    \n",
    "    # 2. Run Merge Process\n",
    "    if success:\n",
    "        merger = ResultMerger()\n",
    "        merger.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5829b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f524ad44",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cd705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Combined_Sector\n",
       "Financials                                         1344\n",
       "Other                                              1099\n",
       "Technology                                          893\n",
       "Healthcare                                          724\n",
       "Consumer Cyclical                                   578\n",
       "                                                   ... \n",
       "Energy, Technology, Healthcare                        1\n",
       "Healthcare, Communication Services, Utilities         1\n",
       "Technology, Healthcare, Real Estate                   1\n",
       "Industrials, Energy, Technology                       1\n",
       "Industrials, Financials, Communication Services       1\n",
       "Name: count, Length: 602, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå combined ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('csv_checkpoint/df_final_result_idx.csv')\n",
    "# df = df[df[\"Sector_Count\"] == 0]\n",
    "df[\"Combined_Sector\"].value_counts()\n",
    "# df = df[[\"Page\", \"Date\", \"Source\",\t\"Title\"\t,\"Link\", \"Content\", \"Combined_Sector\"]]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbc616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glider/sample-project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading source from: csv_checkpoint/df_final_result_idx.csv\n",
      "üìÇ Loading data...\n",
      "‚ú® Found checkpoint: csv_checkpoint/sentiment_final.csv\n",
      "‚úÖ Restored sentiment scores from checkpoint.\n",
      "\n",
      "ü§ñ Starting Model: Qwen/Qwen2.5-14B-Instruct\n",
      "   üìã Remaining items: 2781 / 10812\n",
      "üßπ GPU Memory Cleared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 579/579 [01:03<00:00,  9.10it/s, Materializing param=model.norm.weight]                              \n",
      "Analyzing Qwen2.5-14B-Instruct: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 174/174 [12:37<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ GPU Memory Cleared!\n",
      "\n",
      "‚è© Skipping Meta-Llama-3.1-8B-Instruct (All items processed!)\n",
      "\n",
      "ü§ñ Starting Model: google/gemma-3-12b-it\n",
      "   üìã Remaining items: 10812 / 10812\n",
      "üßπ GPU Memory Cleared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1065/1065 [01:11<00:00, 14.83it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]                       \n",
      "Analyzing gemma-3-12b-it: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 676/676 [58:02<00:00,  5.15s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ GPU Memory Cleared!\n",
      "\n",
      "üéâ Analysis Completed!\n",
      "üíæ Final result saved to: csv_checkpoint/sentiment_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ‡∏õ‡∏¥‡∏î Warning\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error() \n",
    "logging.getLogger(\"transformers.generation_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è CONFIG\n",
    "# ==========================================\n",
    "MODELS_CONFIG = [\n",
    "    # {\"name\": \"microsoft/Phi-3-mini-4k-instruct\", \"weight\": 0.4},\n",
    "    {\"name\": \"Qwen/Qwen2.5-14B-Instruct\", \"weight\": 0.2},\n",
    "    {\"name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"weight\": 0.2},\n",
    "    {\"name\": \"google/gemma-3-12b-it\",\"weight\": 0.2}\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "CSV_CHECKPOINT_DIR = \"csv_checkpoint\"\n",
    "SOURCE_FILE = os.path.join(CSV_CHECKPOINT_DIR, \"df_final_result_idx.csv\")\n",
    "OUTPUT_FILE = os.path.join(CSV_CHECKPOINT_DIR, \"sentiment_final.csv\")\n",
    "\n",
    "# ==========================================\n",
    "# üõ†Ô∏è UTILS\n",
    "# ==========================================\n",
    "def clear_gpu():\n",
    "    # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
    "    if 'model' in globals(): del globals()['model']\n",
    "    if 'tokenizer' in globals(): del globals()['tokenizer']\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ GPU Memory Cleared!\")\n",
    "\n",
    "def create_prompt(text):\n",
    "    return f\"\"\"Analyze the sentiment of this financial news.\n",
    "Consider the impact on the company, sector, or economy mentioned.\n",
    "\n",
    "News: \"{text}\"\n",
    "\n",
    "Return ONLY a JSON object with this format:\n",
    "{{\n",
    "  \"category\": \"Positive\" or \"Negative\" or \"Neutral\",\n",
    "  \"score\": <float number between -1.0 to 1.0>\n",
    "}}\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ MAIN PIPELINE (UPDATED)\n",
    "# ==========================================\n",
    "def run_consensus_pipeline(df_pipe):\n",
    "    print(f\"üìÇ Loading data...\")\n",
    "    df = df_pipe.copy()\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # üîÑ CHECKPOINT SYSTEM: Load existing results if available\n",
    "    # ---------------------------------------------------------\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"‚ú® Found checkpoint: {OUTPUT_FILE}\")\n",
    "        try:\n",
    "            df_existing = pd.read_csv(OUTPUT_FILE)\n",
    "            \n",
    "            # ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Score ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
    "            score_cols = [c for c in df_existing.columns if c.startswith(\"Score_\")]\n",
    "            \n",
    "            # Merge ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Link ‡πÄ‡∏õ‡πá‡∏ô Key (‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ Index ‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤ Data ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)\n",
    "            # ‡πÉ‡∏ä‡πâ Link ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå\n",
    "            if 'Link' in df.columns and 'Link' in df_existing.columns:\n",
    "                # Drop duplicate links in existing data to avoid explosion\n",
    "                df_existing = df_existing.drop_duplicates(subset=['Link'], keep='last')\n",
    "                \n",
    "                # Merge ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Score\n",
    "                cols_to_merge = ['Link'] + score_cols\n",
    "                df = df.merge(df_existing[cols_to_merge], on='Link', how='left', suffixes=('', '_old'))\n",
    "                \n",
    "                # Clean up merge result\n",
    "                for col in score_cols:\n",
    "                    if f\"{col}_old\" in df.columns:\n",
    "                        # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏•‡∏á‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n",
    "                        df[col] = df[col].fillna(df[f\"{col}_old\"])\n",
    "                        df.drop(columns=[f\"{col}_old\"], inplace=True)\n",
    "                \n",
    "                print(f\"‚úÖ Restored sentiment scores from checkpoint.\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No 'Link' column found for merging. Processing from scratch or using index alignment.\")\n",
    "                # Fallback: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Link ‡πÉ‡∏ä‡πâ Index (‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô)\n",
    "                if len(df) == len(df_existing):\n",
    "                    for col in score_cols:\n",
    "                        df[col] = df_existing[col]\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "\n",
    "    # Prepare Text\n",
    "    df['Full_Text'] = (df['Title'].fillna('') + \"\\n\" + df['Content'].fillna('')).str.slice(0, 3000)\n",
    "\n",
    "    for config in MODELS_CONFIG:\n",
    "        MODEL_NAME = config['name']\n",
    "        short_name = MODEL_NAME.split('/')[-1]\n",
    "        col_score = f\"Score_{short_name}\"\n",
    "        \n",
    "        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® Global ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ clear_gpu() ‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô\n",
    "        global model, tokenizer \n",
    "\n",
    "        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ (‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô NaN ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á)\n",
    "        if col_score not in df.columns: \n",
    "            df[col_score] = np.nan\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # üîç SMART FILTER: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô NaN\n",
    "        # ---------------------------------------------------------\n",
    "        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô NaN (‡∏Ñ‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß error ‡∏à‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤)\n",
    "        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß (‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 0.0) ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß\n",
    "        unprocessed_indices = df[df[col_score].isna()].index.tolist()\n",
    "        \n",
    "        if len(unprocessed_indices) == 0:\n",
    "            print(f\"\\n‚è© Skipping {short_name} (All items processed!)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nü§ñ Starting Model: {MODEL_NAME}\")\n",
    "        print(f\"   üìã Remaining items: {len(unprocessed_indices)} / {len(df)}\")\n",
    "        \n",
    "        clear_gpu()\n",
    "        \n",
    "        try:\n",
    "            # Load Tokenizer\n",
    "            try:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Falling back to slow tokenizer...\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "            tokenizer.padding_side = 'left'\n",
    "            if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Load Model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"cuda:0\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Safety Clamp\n",
    "            real_vocab_size = model.get_input_embeddings().weight.shape[0]\n",
    "            MAX_VALID_ID = real_vocab_size - 1\n",
    "\n",
    "            # Loop ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ indices ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥\n",
    "            for i in tqdm(range(0, len(unprocessed_indices), BATCH_SIZE), desc=f\"Analyzing {short_name}\"):\n",
    "                batch_idx = unprocessed_indices[i : i + BATCH_SIZE]\n",
    "                batch_texts = df.loc[batch_idx, 'Full_Text'].tolist()\n",
    "                \n",
    "                prompts = []\n",
    "                for text in batch_texts:\n",
    "                    user_content = create_prompt(text)\n",
    "                    msgs = [{\"role\": \"user\", \"content\": user_content}]\n",
    "                    try:\n",
    "                        formatted_prompt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "                        prompts.append(formatted_prompt)\n",
    "                    except:\n",
    "                        raw_prompt = f\"User: {user_content}\\nAssistant:\"\n",
    "                        prompts.append(raw_prompt)\n",
    "\n",
    "                # Inference\n",
    "                inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(model.device)\n",
    "                \n",
    "                input_ids = inputs['input_ids']\n",
    "                input_ids[input_ids > MAX_VALID_ID] = 0\n",
    "                inputs['input_ids'] = input_ids\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=80, temperature=0.1, do_sample=False)\n",
    "                \n",
    "                decoded = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "                \n",
    "                # Process Results\n",
    "                for idx, resp in zip(batch_idx, decoded):\n",
    "                    score = 0.0 # Default fallback (Neutral)\n",
    "                    try:\n",
    "                        clean = resp.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                        start, end = clean.find('{'), clean.rfind('}') + 1\n",
    "                        if start != -1 and end != -1:\n",
    "                            data = json.loads(clean[start:end])\n",
    "                            score = float(data.get(\"score\", 0.0))\n",
    "                        else:\n",
    "                            # Fallback keyword matching\n",
    "                            if \"positive\" in resp.lower(): score = 0.5\n",
    "                            elif \"negative\" in resp.lower(): score = -0.5\n",
    "                    except: \n",
    "                        pass\n",
    "                    \n",
    "                    # Update DataFrame\n",
    "                    df.at[idx, col_score] = score\n",
    "                \n",
    "                # ---------------------------------------------------------\n",
    "                # üíæ SAVE CHECKPOINT: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏ö Batch\n",
    "                # ---------------------------------------------------------\n",
    "                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n",
    "                df.to_csv(OUTPUT_FILE, index=False)\n",
    "            \n",
    "            del model\n",
    "            del tokenizer\n",
    "            clear_gpu()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed {MODEL_NAME}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# üèÅ EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Folder\n",
    "    if not os.path.exists(CSV_CHECKPOINT_DIR):\n",
    "        os.makedirs(CSV_CHECKPOINT_DIR)\n",
    "        print(f\"üìÅ Created directory: {CSV_CHECKPOINT_DIR}\")\n",
    "\n",
    "    # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Source (df_final_result_idx.csv)\n",
    "    if os.path.exists(SOURCE_FILE):\n",
    "        print(f\"Reading source from: {SOURCE_FILE}\")\n",
    "        df = pd.read_csv(SOURCE_FILE)\n",
    "        \n",
    "        # ‡∏£‡∏±‡∏ô Pipeline\n",
    "        result = run_consensus_pipeline(df)\n",
    "        \n",
    "        print(\"\\nüéâ Analysis Completed!\")\n",
    "        print(f\"üíæ Final result saved to: {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Source file not found: {SOURCE_FILE}\")\n",
    "        print(\"Please upload 'df_final_result_idx.csv' to the 'csv_checkpoint' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d88004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Sector_Dict</th>\n",
       "      <th>Sector_Count</th>\n",
       "      <th>AI_Sector</th>\n",
       "      <th>Combined_Sector</th>\n",
       "      <th>Score_Qwen2.5-14B-Instruct</th>\n",
       "      <th>Score_Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <th>Score_gemma-3-12b-it</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:56:10</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>BofA unveils its top 10 U.S. ideas for Q1 2026</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Bank of America has released ...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>0.044773</td>\n",
       "      <td>{'Financials': 0.04477, 'Healthcare': 0.02544,...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Financials, Healthcare, Energy</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:55:45</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com --¬†Canaccord Genuity analyst Geo...</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>0.049161</td>\n",
       "      <td>{'Consumer Cyclical': 0.04916, 'Energy': 0.02483}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consumer Cyclical, Energy</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:05:02</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Is Reddit the new homepage for the Open Web?</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Reddit is increasingly positi...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.032455</td>\n",
       "      <td>{'Technology': 0.03246}</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 03:24:29</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Trump blocks chips deal, cites security, China...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>WASHINGTON, Jan 2 (Reuters) - President Donald...</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>{'Industrials': 0.02484}</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 01:12:24</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Top hedge funds led by D.E.Shaw, Bridgewater a...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>(Corrects Point72‚Äôs return figures in second b...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>0.146323</td>\n",
       "      <td>{'Financials': 0.14632, 'Utilities': 0.03504}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Financials, Utilities</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page                 Date         Source  \\\n",
       "0     1  2026-01-03 09:56:10  Investing.com   \n",
       "1     1  2026-01-03 09:55:45  Investing.com   \n",
       "2     1  2026-01-03 09:05:02  Investing.com   \n",
       "3     1  2026-01-03 03:24:29        Reuters   \n",
       "4     1  2026-01-03 01:12:24        Reuters   \n",
       "\n",
       "                                               Title  \\\n",
       "0     BofA unveils its top 10 U.S. ideas for Q1 2026   \n",
       "1  Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...   \n",
       "2       Is Reddit the new homepage for the Open Web?   \n",
       "3  Trump blocks chips deal, cites security, China...   \n",
       "4  Top hedge funds led by D.E.Shaw, Bridgewater a...   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.investing.com/news/stock-market-ne...   \n",
       "1  https://www.investing.com/news/stock-market-ne...   \n",
       "2  https://www.investing.com/news/stock-market-ne...   \n",
       "3  https://www.investing.com/news/stock-market-ne...   \n",
       "4  https://www.investing.com/news/stock-market-ne...   \n",
       "\n",
       "                                             Content             Sector  \\\n",
       "0  Investing.com -- Bank of America has released ...         Financials   \n",
       "1  Investing.com --¬†Canaccord Genuity analyst Geo...  Consumer Cyclical   \n",
       "2  Investing.com -- Reddit is increasingly positi...         Technology   \n",
       "3  WASHINGTON, Jan 2 (Reuters) - President Donald...        Industrials   \n",
       "4  (Corrects Point72‚Äôs return figures in second b...         Financials   \n",
       "\n",
       "   Confidence                                        Sector_Dict  \\\n",
       "0    0.044773  {'Financials': 0.04477, 'Healthcare': 0.02544,...   \n",
       "1    0.049161  {'Consumer Cyclical': 0.04916, 'Energy': 0.02483}   \n",
       "2    0.032455                            {'Technology': 0.03246}   \n",
       "3    0.024836                           {'Industrials': 0.02484}   \n",
       "4    0.146323      {'Financials': 0.14632, 'Utilities': 0.03504}   \n",
       "\n",
       "   Sector_Count AI_Sector                 Combined_Sector  \\\n",
       "0             3       NaN  Financials, Healthcare, Energy   \n",
       "1             2       NaN       Consumer Cyclical, Energy   \n",
       "2             1       NaN                      Technology   \n",
       "3             1       NaN                     Industrials   \n",
       "4             2       NaN           Financials, Utilities   \n",
       "\n",
       "   Score_Qwen2.5-14B-Instruct  Score_Meta-Llama-3.1-8B-Instruct  \\\n",
       "0                        0.60                              0.65   \n",
       "1                        0.85                              0.85   \n",
       "2                        0.85                              0.83   \n",
       "3                       -0.75                             -0.70   \n",
       "4                        0.85                              0.85   \n",
       "\n",
       "   Score_gemma-3-12b-it  \n",
       "0                  0.60  \n",
       "1                  0.85  \n",
       "2                  0.85  \n",
       "3                 -0.70  \n",
       "4                  0.90  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save to csv (drop Fulltext)\n",
    "# result = result.drop(columns=['Full_Text' , 'Score_gemma-3-4b-it'])\n",
    "result.to_csv('csv_checkpoint/sentiment_final.csv', index=False)\n",
    "result.isnull().sum()\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8feed6",
   "metadata": {},
   "source": [
    "## News Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766298f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Main Data from csv_checkpoint/sentiment_final.csv...\n",
      "üîé Found existing output file: csv_checkpoint/news_summary.csv\n",
      "   ‚úÖ Recovered 10812 existing summaries.\n",
      "\n",
      "üìä Status Report:\n",
      "   - Total News: 10812\n",
      "   - Already Done: 10812\n",
      "   - To Do (GPU): 0\n",
      "\n",
      "‚ú® All news already summarized! Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è SYSTEM CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct\" \n",
    "BATCH_SIZE = 32\n",
    "MAX_OUTPUT_TOKENS = 60 \n",
    "\n",
    "# Files\n",
    "INPUT_FILE = 'csv_checkpoint/sentiment_final.csv'\n",
    "OUTPUT_FILE = 'csv_checkpoint/news_summary.csv'\n",
    "\n",
    "# ==========================================\n",
    "# üõ†Ô∏è UTILITIES: GPU MANAGER\n",
    "# ==========================================\n",
    "def clear_resources():\n",
    "    \"\"\"‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ GPU ‡πÅ‡∏ö‡∏ö‡∏´‡∏°‡∏î‡∏à‡∏î\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ GPU Memory Cleared\")\n",
    "\n",
    "# ==========================================\n",
    "# üß† CORE AI ENGINE (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)\n",
    "# ==========================================\n",
    "class NewsSummarizer:\n",
    "    def __init__(self, model_name):\n",
    "        print(f\"ü§ñ Loading Model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\" \n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"cuda:0\", \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_batch(self, titles, contents, batch_size):\n",
    "        prompts = []\n",
    "        for t, c in zip(titles, contents):\n",
    "            prompt = f\"\"\"Task: Summarize the financial news into 1 sentence.\n",
    "News: {t} - {str(c)[:1000]}...\n",
    "Summary:\"\"\"\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            prompts.append(formatted_prompt)\n",
    "\n",
    "        all_summaries = []\n",
    "        total_items = len(prompts)\n",
    "        \n",
    "        print(f\"üöÄ Starting Batch Processing: {total_items} items (Batch Size: {batch_size})\")\n",
    "\n",
    "        for i in tqdm(range(0, total_items, batch_size), desc=\"Summarizing\"):\n",
    "            batch_prompts = prompts[i : i + batch_size]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch_prompts, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens=MAX_OUTPUT_TOKENS,\n",
    "                    temperature=0.1, \n",
    "                    do_sample=False, \n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            generated_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
    "            decoded_batch = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            clean_batch = [txt.strip().replace('\\n', ' ') for txt in decoded_batch]\n",
    "            all_summaries.extend(clean_batch)\n",
    "\n",
    "        return all_summaries\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ MAIN PIPELINE (UPDATED)\n",
    "# ==========================================\n",
    "def run_pipeline():\n",
    "    # 1. Load Main Input Data\n",
    "    print(f\"üìÇ Loading Main Data from {INPUT_FILE}...\")\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"‚ùå Input file {INPUT_FILE} not found. Please run the previous step first.\")\n",
    "        return\n",
    "    \n",
    "    df_main = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # 2. Check for Existing Output (The Cache)\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"üîé Found existing output file: {OUTPUT_FILE}\")\n",
    "        df_existing = pd.read_csv(OUTPUT_FILE)\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Column ‡∏Ñ‡∏£‡∏ö‡πÑ‡∏´‡∏°\n",
    "        if 'Link' in df_existing.columns and 'Short_Ans' in df_existing.columns:\n",
    "            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dictionary {Link: Short_Ans} ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤\n",
    "            # ‡πÉ‡∏ä‡πâ drop_duplicates ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Link ‡πÄ‡∏õ‡πá‡∏ô Unique Key\n",
    "            existing_map = df_existing.dropna(subset=['Short_Ans']).drop_duplicates(subset=['Link']).set_index('Link')['Short_Ans'].to_dict()\n",
    "            \n",
    "            # Map ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏™‡πà df_main (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Link ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô ‡∏à‡∏∞‡πÑ‡∏î‡πâ Summary ‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡πÄ‡∏•‡∏¢)\n",
    "            df_main['Short_Ans'] = df_main['Link'].map(existing_map)\n",
    "            \n",
    "            # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô\n",
    "            found_count = df_main['Short_Ans'].notna().sum()\n",
    "            print(f\"   ‚úÖ Recovered {found_count} existing summaries.\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è Existing file structure incorrect. Will re-process all.\")\n",
    "            df_main['Short_Ans'] = None\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è No existing output found. Starting fresh.\")\n",
    "        df_main['Short_Ans'] = None\n",
    "\n",
    "    # 3. Identify \"To-Do\" Items (Filter rows with NO summary)\n",
    "    # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç: ‡πÄ‡∏õ‡πá‡∏ô NaN ‡∏´‡∏£‡∏∑‡∏≠ ‡πÄ‡∏õ‡πá‡∏ô string ‡∏ß‡πà‡∏≤‡∏á\n",
    "    mask_todo = df_main['Short_Ans'].isna() | (df_main['Short_Ans'] == \"\")\n",
    "    df_todo = df_main[mask_todo]\n",
    "    \n",
    "    total_rows = len(df_main)\n",
    "    todo_rows = len(df_todo)\n",
    "    \n",
    "    print(f\"\\nüìä Status Report:\")\n",
    "    print(f\"   - Total News: {total_rows}\")\n",
    "    print(f\"   - Already Done: {total_rows - todo_rows}\")\n",
    "    print(f\"   - To Do (GPU): {todo_rows}\")\n",
    "\n",
    "    # 4. Conditional Execution\n",
    "    if todo_rows == 0:\n",
    "        print(\"\\n‚ú® All news already summarized! Nothing to do.\")\n",
    "        # Save again just to be sure files are synced\n",
    "        df_main.to_csv(OUTPUT_FILE, index=False)\n",
    "        return\n",
    "\n",
    "    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏´‡∏•‡∏î Model ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥\n",
    "    summarizer = NewsSummarizer(MODEL_NAME)\n",
    "    \n",
    "    try:\n",
    "        # 5. Run Batch Summarization (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ df_todo)\n",
    "        print(\"\\nüöÄ Processing new items...\")\n",
    "        new_summaries = summarizer.generate_batch(\n",
    "            df_todo['Title'].tolist(), \n",
    "            df_todo['Content'].fillna('').tolist(), \n",
    "            BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # 6. Merge Results Back\n",
    "        # ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏î‡∏¥‡∏° (Locate by mask)\n",
    "        df_main.loc[mask_todo, 'Short_Ans'] = new_summaries\n",
    "        \n",
    "        # 7. Save Result\n",
    "        df_main.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"\\n‚úÖ Pipeline Complete! Saved updated data to {OUTPUT_FILE}\")\n",
    "        \n",
    "        # Show sample of NEW summaries\n",
    "        print(\"\\nSample of NEW summaries:\")\n",
    "        print(df_main.loc[mask_todo, ['Title', 'Short_Ans']].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # 8. Cleanup\n",
    "        if 'summarizer' in locals():\n",
    "            del summarizer\n",
    "        clear_resources()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a7c782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Sector_Dict</th>\n",
       "      <th>Sector_Count</th>\n",
       "      <th>AI_Sector</th>\n",
       "      <th>Combined_Sector</th>\n",
       "      <th>Score_Qwen2.5-14B-Instruct</th>\n",
       "      <th>Score_Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <th>Score_gemma-3-12b-it</th>\n",
       "      <th>Short_Ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:56:10</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>BofA unveils its top 10 U.S. ideas for Q1 2026</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Bank of America has released ...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>0.044773</td>\n",
       "      <td>{'Financials': 0.04477, 'Healthcare': 0.02544,...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Financials, Healthcare, Energy</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>Bank of America has unveiled its top 10 U.S. s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:55:45</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com --¬†Canaccord Genuity analyst Geo...</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>0.049161</td>\n",
       "      <td>{'Consumer Cyclical': 0.04916, 'Energy': 0.02483}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consumer Cyclical, Energy</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>Canaccord Genuity analyst George Gianarikas pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 09:05:02</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>Is Reddit the new homepage for the Open Web?</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Investing.com -- Reddit is increasingly positi...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.032455</td>\n",
       "      <td>{'Technology': 0.03246}</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>Needham analyst Laura Martin suggests that Red...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 03:24:29</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Trump blocks chips deal, cites security, China...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>WASHINGTON, Jan 2 (Reuters) - President Donald...</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>{'Industrials': 0.02484}</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>President Trump has blocked a $3 million chip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-03 01:12:24</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Top hedge funds led by D.E.Shaw, Bridgewater a...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>(Corrects Point72‚Äôs return figures in second b...</td>\n",
       "      <td>Financials</td>\n",
       "      <td>0.146323</td>\n",
       "      <td>{'Financials': 0.14632, 'Utilities': 0.03504}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Financials, Utilities</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Top hedge funds such as D.E. Shaw, Bridgewater...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page                 Date         Source  \\\n",
       "0     1  2026-01-03 09:56:10  Investing.com   \n",
       "1     1  2026-01-03 09:55:45  Investing.com   \n",
       "2     1  2026-01-03 09:05:02  Investing.com   \n",
       "3     1  2026-01-03 03:24:29        Reuters   \n",
       "4     1  2026-01-03 01:12:24        Reuters   \n",
       "\n",
       "                                               Title  \\\n",
       "0     BofA unveils its top 10 U.S. ideas for Q1 2026   \n",
       "1  Canaccord‚Äôs says 2026 is likely to be ‚Äôa bount...   \n",
       "2       Is Reddit the new homepage for the Open Web?   \n",
       "3  Trump blocks chips deal, cites security, China...   \n",
       "4  Top hedge funds led by D.E.Shaw, Bridgewater a...   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://www.investing.com/news/stock-market-ne...   \n",
       "1  https://www.investing.com/news/stock-market-ne...   \n",
       "2  https://www.investing.com/news/stock-market-ne...   \n",
       "3  https://www.investing.com/news/stock-market-ne...   \n",
       "4  https://www.investing.com/news/stock-market-ne...   \n",
       "\n",
       "                                             Content             Sector  \\\n",
       "0  Investing.com -- Bank of America has released ...         Financials   \n",
       "1  Investing.com --¬†Canaccord Genuity analyst Geo...  Consumer Cyclical   \n",
       "2  Investing.com -- Reddit is increasingly positi...         Technology   \n",
       "3  WASHINGTON, Jan 2 (Reuters) - President Donald...        Industrials   \n",
       "4  (Corrects Point72‚Äôs return figures in second b...         Financials   \n",
       "\n",
       "   Confidence                                        Sector_Dict  \\\n",
       "0    0.044773  {'Financials': 0.04477, 'Healthcare': 0.02544,...   \n",
       "1    0.049161  {'Consumer Cyclical': 0.04916, 'Energy': 0.02483}   \n",
       "2    0.032455                            {'Technology': 0.03246}   \n",
       "3    0.024836                           {'Industrials': 0.02484}   \n",
       "4    0.146323      {'Financials': 0.14632, 'Utilities': 0.03504}   \n",
       "\n",
       "   Sector_Count AI_Sector                 Combined_Sector  \\\n",
       "0             3       NaN  Financials, Healthcare, Energy   \n",
       "1             2       NaN       Consumer Cyclical, Energy   \n",
       "2             1       NaN                      Technology   \n",
       "3             1       NaN                     Industrials   \n",
       "4             2       NaN           Financials, Utilities   \n",
       "\n",
       "   Score_Qwen2.5-14B-Instruct  Score_Meta-Llama-3.1-8B-Instruct  \\\n",
       "0                        0.60                              0.65   \n",
       "1                        0.85                              0.85   \n",
       "2                        0.85                              0.83   \n",
       "3                       -0.75                             -0.70   \n",
       "4                        0.85                              0.85   \n",
       "\n",
       "   Score_gemma-3-12b-it                                          Short_Ans  \n",
       "0                  0.60  Bank of America has unveiled its top 10 U.S. s...  \n",
       "1                  0.85  Canaccord Genuity analyst George Gianarikas pr...  \n",
       "2                  0.85  Needham analyst Laura Martin suggests that Red...  \n",
       "3                 -0.70  President Trump has blocked a $3 million chip ...  \n",
       "4                  0.90  Top hedge funds such as D.E. Shaw, Bridgewater...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"csv_checkpoint/news_summary.csv\").head()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f31ee4",
   "metadata": {},
   "source": [
    "## AI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5039c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Data...\n",
      "‚úÖ Data Ready. Analyzing History: ['2026-01-08', '2026-01-09', '2026-01-10']\n",
      "\n",
      "==================================================\n",
      "ü§ñ Loading Model: Qwen/Qwen2.5-14B-Instruct (Qwen)...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 579/579 [00:47<00:00, 12.27it/s, Materializing param=model.norm.weight]                               \n",
      "üìÖ Processing Days (Qwen):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è CONFIGURATION & MODEL WEIGHTS\n",
    "# ==========================================\n",
    "LOOKBACK_DAYS = 7 \n",
    "ANALYSIS_RANGE = 3  # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 2 ‡∏ß‡∏±‡∏ô (‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)\n",
    "\n",
    "# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠\n",
    "MODEL_CONFIGS = [\n",
    "    #{\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"short_name\": \"Deepseek\", \"weight\": 0.4},\n",
    "    {\"name\": \"Qwen/Qwen2.5-14B-Instruct\", \"short_name\": \"Qwen\", \"weight\": 0.34},\n",
    "    {\"name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"short_name\": \"Llama\", \"weight\": 0.33},\n",
    "    {\"name\": \"google/gemma-3-12b-it\", \"short_name\": \"Gemma\", \"weight\": 0.33} \n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 1. üì• LOAD & PREPARE DATA\n",
    "# ==========================================\n",
    "print(\"üìÇ Loading Data...\")\n",
    "try:\n",
    "    df = pd.read_csv('csv_checkpoint/news_summary.csv')\n",
    "    \n",
    "    if 'Short_Ans' not in df.columns: df['Short_Ans'] = df['Content']\n",
    "    if 'Date' not in df.columns: \n",
    "        df['Date'] = [datetime.now() - timedelta(days=x%12) for x in range(len(df))]\n",
    "    else:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Explode Sectors\n",
    "    df['Sector_List'] = df['Combined_Sector'].astype(str).str.split(',')\n",
    "    expanded_df = df.explode('Sector_List')\n",
    "    expanded_df['Target_Sector'] = expanded_df['Sector_List'].str.strip()\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á\n",
    "    latest_db_date = df['Date'].max()\n",
    "    target_dates = [latest_db_date - timedelta(days=i) for i in range(ANALYSIS_RANGE)]\n",
    "    target_dates.reverse() # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡πà‡∏≤ -> ‡πÉ‡∏´‡∏°‡πà\n",
    "    \n",
    "    print(f\"‚úÖ Data Ready. Analyzing History: {[d.strftime('%Y-%m-%d') for d in target_dates]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error Loading Data: {e}\")\n",
    "    target_dates = []\n",
    "    expanded_df = pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# 2. üß† HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "def get_sector_context(sector_name, full_df):\n",
    "    \"\"\"‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sector ‡∏ô‡∏±‡πâ‡∏ô‡πÜ\"\"\"\n",
    "    sector_df = full_df[full_df['Target_Sector'] == sector_name].sort_values(by='Date', ascending=False)\n",
    "    \n",
    "    # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Window ‡∏ô‡∏µ‡πâ\n",
    "    news_count = len(sector_df)\n",
    "    \n",
    "    # Simple weighted score calc\n",
    "    total_weight = sector_df['Time_Weight'].sum()\n",
    "    weighted_avg_score = (sector_df['Weighted_Score'].sum() / total_weight) if total_weight > 0 else 0\n",
    "    \n",
    "    news_context = \"\"\n",
    "    for _, row in sector_df.iterrows(): # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 5 ‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î context string\n",
    "        d_str = row['Date'].strftime('%Y-%m-%d')\n",
    "        news_context += f\"- {d_str}: {row.get('Title', 'N/A')} -> {str(row.get('Short_Ans', ''))[:150]}...\\n\"\n",
    "        \n",
    "    return news_context, weighted_avg_score, news_count\n",
    "\n",
    "def parse_llm_response(response_text):\n",
    "    \"\"\"‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á JSON ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\"\"\"\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if match:\n",
    "            data = json.loads(match.group())\n",
    "            return data.get('score', 5.0), data.get('analysis', 'No analysis'), data.get('outlook', 'Neutral')\n",
    "    except:\n",
    "        pass\n",
    "    return 5.0, \"Error parsing output\", \"Neutral\"\n",
    "\n",
    "# ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà -> Sector -> Model\n",
    "history_results = {}\n",
    "\n",
    "# ==========================================\n",
    "# 3. üîÑ MODEL LOOP\n",
    "# ==========================================\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    model_name = config['name']\n",
    "    short_name = config['short_name']\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"ü§ñ Loading Model: {model_name} ({short_name})...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Load Model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda:0\",\n",
    "            trust_remote_code=True,\n",
    "            # token=\"YOUR_HUGGINGFACE_TOKEN\" # ‡πÉ‡∏™‡πà token ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "        )\n",
    "        \n",
    "        # Loop Dates\n",
    "        for target_date in tqdm(target_dates, desc=f\"üìÖ Processing Days ({short_name})\"):\n",
    "            target_date_str = target_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Filter Data\n",
    "            start_window = target_date - timedelta(days=LOOKBACK_DAYS)\n",
    "            daily_df = expanded_df[\n",
    "                (expanded_df['Date'] <= target_date) & \n",
    "                (expanded_df['Date'] >= start_window)\n",
    "            ].copy()\n",
    "            \n",
    "            if daily_df.empty: continue\n",
    "\n",
    "            # Time Weight Calculation\n",
    "            daily_df['Days_Ago'] = (target_date - daily_df['Date']).dt.days\n",
    "            daily_df['Time_Weight'] = daily_df['Days_Ago'].apply(lambda d: max(0.1, 1 - (d / (LOOKBACK_DAYS + 1))))\n",
    "            \n",
    "            if 'Consensus_Score' not in daily_df.columns:\n",
    "                 score_cols = [c for c in daily_df.columns if 'Score_' in c]\n",
    "                 if score_cols: daily_df['Consensus_Score'] = daily_df[score_cols].mean(axis=1)\n",
    "                 else: daily_df['Consensus_Score'] = 0\n",
    "            \n",
    "            daily_df['Weighted_Score'] = daily_df['Consensus_Score'] * daily_df['Time_Weight']\n",
    "            \n",
    "            unique_sectors = daily_df['Target_Sector'].dropna().unique()\n",
    "\n",
    "            # Loop Sectors\n",
    "            for sector in unique_sectors:\n",
    "                if len(str(sector)) < 2: continue\n",
    "                \n",
    "                # ‚úÖ ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ news_count ‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢\n",
    "                news_context, q_score, news_count = get_sector_context(sector, daily_df)\n",
    "                \n",
    "                # Prompt\n",
    "                prompt = f\"\"\"\n",
    "Role: Senior Financial Analyst.\n",
    "Task: Analyze the market sentiment for '{sector}' with a focus on REAL-TIME MOMENTUM.\n",
    "\n",
    "Quantitative Signal:\n",
    "- Time-Weighted Sentiment Score: {q_score:.2f} (Scale: -1.0 to +1.0)\n",
    " (This score prioritizes recent news over older news)\n",
    "\n",
    "News Feed (Sorted by Recency - Newest First):\n",
    "{news_context}\n",
    "\n",
    "Instructions:\n",
    "1. **Recency Bias:** Give significantly more weight to news from the last 2-3 days (Top of the list). Old news (7-10 days ago) should be treated as \"Context\" but not drivers.\n",
    "2. **Outlook:** Determine 'Bullish', 'Bearish', or 'Neutral'.\n",
    "3. **Score:** Score: Assign a precise sentiment score (0.0 - 10.0), e.g., 7.5 or 4.2.\n",
    "4. **Analysis:** Write a short executive summary (Max 3 sentences). Explicitly mention if the sentiment has shifted recently (e.g., \"Started week strong but ended weak\").\n",
    "\n",
    "Output strictly in JSON format:\n",
    "{{\n",
    "  \"outlook\": \"Bearish\" or \"Bullish\" or \"Neutral\",\n",
    "  \"score\": <float 0-10>,\n",
    "  \"analysis\": \"<Max 3 sentences>\"\n",
    "}}\n",
    "\"\"\"\n",
    "                # Generate\n",
    "                try:\n",
    "                    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                    try:\n",
    "                        text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "                    except:\n",
    "                        text_input = f\"User: {prompt}\\n\\nAssistant:\"\n",
    "                    \n",
    "                    inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.35)\n",
    "                        \n",
    "                    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "                    score, analysis, outlook = parse_llm_response(response)\n",
    "                    \n",
    "                    # Store Results\n",
    "                    if target_date_str not in history_results: history_results[target_date_str] = {}\n",
    "                    if sector not in history_results[target_date_str]: history_results[target_date_str][sector] = {}\n",
    "                    \n",
    "                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Model\n",
    "                    history_results[target_date_str][sector][short_name] = {\n",
    "                        \"score\": float(score),\n",
    "                        \"analysis\": analysis,\n",
    "                        \"outlook\": outlook\n",
    "                    }\n",
    "                    \n",
    "                    # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß (‡∏ó‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)\n",
    "                    history_results[target_date_str][sector]['news_volume'] = news_count\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"üßπ Unloaded {short_name} to free VRAM.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to run {model_name}: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. üìä AGGREGATION & EXPORT\n",
    "# ==========================================\n",
    "print(\"\\nüßÆ Aggregating Daily History...\")\n",
    "\n",
    "final_rows = []\n",
    "\n",
    "for date_str, sectors_data in history_results.items():\n",
    "    for sector, models_data in sectors_data.items():\n",
    "        # ‚úÖ ‡∏î‡∏∂‡∏á News_Volume ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤\n",
    "        news_vol = models_data.get('news_volume', 0)\n",
    "\n",
    "        row_data = {\n",
    "            'Report_Date': date_str,\n",
    "            'Sector': sector,\n",
    "            'News_Volume': news_vol  # ‚úÖ ‡πÉ‡∏™‡πà Column ‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ\n",
    "        }\n",
    "        \n",
    "        total_weighted_score = 0\n",
    "        total_model_weight = 0\n",
    "        \n",
    "        for config in MODEL_CONFIGS:\n",
    "            s_name = config['short_name']\n",
    "            m_weight = config['weight']\n",
    "            \n",
    "            res = models_data.get(s_name, {\"score\": 5.0, \"analysis\": \"N/A\", \"outlook\": \"N/A\"})\n",
    "            \n",
    "            row_data[f'Score_{s_name}'] = res['score']\n",
    "            row_data[f'Reason_{s_name}'] = res['analysis']\n",
    "            \n",
    "            total_weighted_score += res['score'] * m_weight\n",
    "            total_model_weight += m_weight\n",
    "        \n",
    "        final_score = total_weighted_score / total_model_weight if total_model_weight > 0 else 5.0\n",
    "        row_data['Final_Daily_Score'] = round(final_score, 2)\n",
    "        \n",
    "        if final_score >= 6.5: row_data['Final_Outlook'] = 'Bullish'\n",
    "        elif final_score <= 3.5: row_data['Final_Outlook'] = 'Bearish'\n",
    "        else: row_data['Final_Outlook'] = 'Neutral'\n",
    "        \n",
    "        final_rows.append(row_data)\n",
    "\n",
    "df_history = pd.DataFrame(final_rows)\n",
    "\n",
    "if not df_history.empty:\n",
    "    # ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö Column ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°\n",
    "    cols = ['Report_Date', 'Sector', 'News_Volume'] + [c for c in df_history.columns if c not in ['Report_Date', 'Sector', 'News_Volume']]\n",
    "    df_history = df_history[cols]\n",
    "    \n",
    "    df_history = df_history.sort_values(by=['Report_Date', 'Final_Daily_Score'], ascending=[True, False])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" üèÜ FINAL 7-DAY HISTORY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_history[['Report_Date', 'Sector', 'News_Volume', 'Final_Daily_Score', 'Final_Outlook']].head(10))\n",
    "\n",
    "    df_history.to_csv('csv_checkpoint/sector_daily_history_7days.csv', index=False)\n",
    "    print(\"\\n‚úÖ Saved history to 'sector_daily_history_7days.csv'\")\n",
    "else:\n",
    "    print(\"‚ùå No history generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2801a082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report_Date</th>\n",
       "      <th>Sector</th>\n",
       "      <th>News_Volume</th>\n",
       "      <th>Score_Qwen</th>\n",
       "      <th>Reason_Qwen</th>\n",
       "      <th>Score_Llama</th>\n",
       "      <th>Reason_Llama</th>\n",
       "      <th>Score_Gemma</th>\n",
       "      <th>Reason_Gemma</th>\n",
       "      <th>Final_Daily_Score</th>\n",
       "      <th>Final_Outlook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Technology</td>\n",
       "      <td>261</td>\n",
       "      <td>8.3</td>\n",
       "      <td>The technology sector has seen a strong moment...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>The recent surge in AI-related news, particula...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The Technology sector currently exhibits a str...</td>\n",
       "      <td>8.10</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Defense</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The most recent news indicates a significant m...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>The recent news of Lockheed Martin securing a ...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The market sentiment for Defense is currently ...</td>\n",
       "      <td>6.75</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>170</td>\n",
       "      <td>5.3</td>\n",
       "      <td>The healthcare sector shows mixed signals, wit...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>The recent news has been dominated by negative...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Market sentiment for Healthcare is currently b...</td>\n",
       "      <td>5.43</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Financials</td>\n",
       "      <td>341</td>\n",
       "      <td>5.3</td>\n",
       "      <td>The market sentiment for financials remains la...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The recent news on US oil companies seeking gu...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Current market sentiment for Financials appear...</td>\n",
       "      <td>5.30</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Other</td>\n",
       "      <td>117</td>\n",
       "      <td>2.8</td>\n",
       "      <td>The market sentiment for 'Other' has shifted t...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>The market sentiment for 'Other' is bearish du...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Current market sentiment for 'Other' is decide...</td>\n",
       "      <td>4.58</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>165</td>\n",
       "      <td>5.2</td>\n",
       "      <td>The recent news feed indicates a mixed sentime...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>The recent news suggests a bearish trend in th...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Recent news paints a mixed but ultimately bear...</td>\n",
       "      <td>4.41</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>161</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The Industrials sector has shown a recent shif...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The Industrials sector is experiencing a beari...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Recent news indicates a slightly bearish senti...</td>\n",
       "      <td>4.16</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>115</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The market sentiment for Basic Materials has s...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The market sentiment for Basic Materials is be...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Recent market sentiment for Basic Materials le...</td>\n",
       "      <td>4.16</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>69</td>\n",
       "      <td>3.8</td>\n",
       "      <td>The Consumer Defensive sector experienced a sh...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The recent news on Conagra, General Mills, and...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Recent market sentiment for Consumer Defensive...</td>\n",
       "      <td>4.06</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>83</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The recent news indicates a shift towards a be...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The Communication Services sector has taken a ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The Communication Services sector currently ex...</td>\n",
       "      <td>3.96</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>135</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The recent news feed shows a predominantly neg...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>The Utilities sector is experiencing a bearish...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Current market sentiment for Utilities leans b...</td>\n",
       "      <td>3.83</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>20</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The real estate sector experienced a sharp dow...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>The recent news of President Trump's proposal ...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Current market sentiment for real estate is le...</td>\n",
       "      <td>3.70</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>Energy</td>\n",
       "      <td>201</td>\n",
       "      <td>3.2</td>\n",
       "      <td>The energy sector's sentiment has turned beari...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>The recent news from the US government's stanc...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The energy market sentiment is currently leani...</td>\n",
       "      <td>3.40</td>\n",
       "      <td>Bearish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>176</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The healthcare sector has shown significant mo...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The recent news of Merck's acquisition talks w...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>Healthcare sentiment is currently strongly bul...</td>\n",
       "      <td>6.94</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Defense</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The most recent news indicates a significant d...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>The recent news of Lockheed Martin securing a ...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The market sentiment for Defense is currently ...</td>\n",
       "      <td>6.75</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>171</td>\n",
       "      <td>7.2</td>\n",
       "      <td>The Consumer Cyclical sector showed strong mom...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>The recent surge in Uniqlo's parent company, F...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The market sentiment for Consumer Cyclicals is...</td>\n",
       "      <td>6.41</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>25</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The real estate sector experienced a significa...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>The recent news has been dominated by negative...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Despite a slightly negative Time-Weighted Sent...</td>\n",
       "      <td>5.72</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>123</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The market sentiment for Basic Materials has t...</td>\n",
       "      <td>6.2</td>\n",
       "      <td>The recent news of BlueScope's rejection of a ...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Current market sentiment for Basic Materials l...</td>\n",
       "      <td>5.48</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Financials</td>\n",
       "      <td>354</td>\n",
       "      <td>5.3</td>\n",
       "      <td>The recent news feed does not heavily favor ei...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The recent news of Strava's IPO filing and Mac...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Current market sentiment for Financials appear...</td>\n",
       "      <td>5.30</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>149</td>\n",
       "      <td>5.3</td>\n",
       "      <td>The market sentiment for utilities remains lar...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>The Utilities sector is experiencing a bearish...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Current market sentiment for Utilities appears...</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>188</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The Industrials sector experienced mixed signa...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The Industrials sector is experiencing a beari...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Despite some positive news regarding General E...</td>\n",
       "      <td>4.94</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Energy</td>\n",
       "      <td>217</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The recent news indicates a mixed sentiment to...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>The energy sector is experiencing a bearish se...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Current market sentiment for Energy is leaning...</td>\n",
       "      <td>4.81</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Other</td>\n",
       "      <td>122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The recent news feed does not predominantly fa...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>The market sentiment for 'Other' has turned be...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>Market sentiment for 'Other' is currently bull...</td>\n",
       "      <td>4.77</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>89</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The market sentiment for Communication Service...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>The Communication Services sector is experienc...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The Communication Services sector currently ex...</td>\n",
       "      <td>4.57</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>79</td>\n",
       "      <td>5.3</td>\n",
       "      <td>The Consumer Defensive sector shows mixed sign...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>The Consumer Defensive sector is experiencing ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Current market sentiment for Consumer Defensiv...</td>\n",
       "      <td>4.11</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2026-01-09</td>\n",
       "      <td>Technology</td>\n",
       "      <td>291</td>\n",
       "      <td>3.5</td>\n",
       "      <td>The recent news primarily highlights financial...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The recent news from xAI's significant cash bu...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Recent news heavily emphasizes negative develo...</td>\n",
       "      <td>3.96</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>119</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The market for Basic Materials shows a positiv...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>The Basic Materials sector has shown a positiv...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Current market sentiment for Basic Materials i...</td>\n",
       "      <td>7.47</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Other</td>\n",
       "      <td>120</td>\n",
       "      <td>8.2</td>\n",
       "      <td>The market sentiment for 'Other' has shown a s...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The market sentiment for 'Other' is currently ...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Market sentiment for 'Other' is currently stro...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Defense</td>\n",
       "      <td>1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>The most recent news indicates a significant m...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The recent news of Lockheed Martin securing a ...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Current market sentiment for the Defense secto...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>186</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The healthcare sector shows mixed signals with...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>The recent news on Edwards Lifesciences' acqui...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The healthcare sector currently exhibits a bul...</td>\n",
       "      <td>6.03</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Energy</td>\n",
       "      <td>229</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The market sentiment for Energy has been mixed...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>The recent news on the Energy sector has turne...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Market sentiment for Energy is currently bulli...</td>\n",
       "      <td>5.80</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>85</td>\n",
       "      <td>5.2</td>\n",
       "      <td>The recent news feed shows mixed signals withi...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>The recent news on Chipotle's potential to bec...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Current market sentiment for Consumer Defensiv...</td>\n",
       "      <td>5.50</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>194</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The Industrials sector experienced mixed signa...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>The Industrials sector is experiencing a beari...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The Industrials sector currently presents a mi...</td>\n",
       "      <td>5.47</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>158</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The market sentiment for Utilities has remaine...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>The Utilities sector's sentiment has taken a b...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Current market sentiment for Utilities is lean...</td>\n",
       "      <td>5.37</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Technology</td>\n",
       "      <td>294</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The market sentiment for technology has shown ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The recent news suggests a bearish sentiment f...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>Current market sentiment for Technology is lea...</td>\n",
       "      <td>5.27</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>24</td>\n",
       "      <td>7.8</td>\n",
       "      <td>The real estate sector experienced a significa...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>The recent news of Trump's order for Freddie a...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Despite some positive movement in mortgage and...</td>\n",
       "      <td>4.96</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Financials</td>\n",
       "      <td>361</td>\n",
       "      <td>5.2</td>\n",
       "      <td>The market sentiment for financials remains la...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>The recent news has been dominated by negative...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Current market sentiment for Financials is lea...</td>\n",
       "      <td>4.84</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>192</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The Consumer Cyclical sector shows mixed signa...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The recent news feed suggests a bearish sentim...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Despite some positive momentum from Paranovus,...</td>\n",
       "      <td>4.74</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>83</td>\n",
       "      <td>5.8</td>\n",
       "      <td>The recent news feed shows a mixed impact on t...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>The Communication Services sector is experienc...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Current market sentiment for Communication Ser...</td>\n",
       "      <td>4.74</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Report_Date                  Sector  News_Volume  Score_Qwen  \\\n",
       "0   2026-01-08              Technology          261         8.3   \n",
       "1   2026-01-08                 Defense            1         5.0   \n",
       "2   2026-01-08              Healthcare          170         5.3   \n",
       "3   2026-01-08              Financials          341         5.3   \n",
       "4   2026-01-08                   Other          117         2.8   \n",
       "5   2026-01-08       Consumer Cyclical          165         5.2   \n",
       "6   2026-01-08             Industrials          161         3.5   \n",
       "7   2026-01-08         Basic Materials          115         3.5   \n",
       "8   2026-01-08      Consumer Defensive           69         3.8   \n",
       "9   2026-01-08  Communication Services           83         3.5   \n",
       "10  2026-01-08               Utilities          135         3.5   \n",
       "11  2026-01-08             Real Estate           20         3.5   \n",
       "12  2026-01-08                  Energy          201         3.2   \n",
       "13  2026-01-09              Healthcare          176         7.8   \n",
       "14  2026-01-09                 Defense            1         5.0   \n",
       "15  2026-01-09       Consumer Cyclical          171         7.2   \n",
       "16  2026-01-09             Real Estate           25         7.8   \n",
       "17  2026-01-09         Basic Materials          123         3.5   \n",
       "18  2026-01-09              Financials          354         5.3   \n",
       "19  2026-01-09               Utilities          149         5.3   \n",
       "20  2026-01-09             Industrials          188         5.8   \n",
       "21  2026-01-09                  Energy          217         5.8   \n",
       "22  2026-01-09                   Other          122         5.0   \n",
       "23  2026-01-09  Communication Services           89         5.0   \n",
       "24  2026-01-09      Consumer Defensive           79         5.3   \n",
       "25  2026-01-09              Technology          291         3.5   \n",
       "26  2026-01-10         Basic Materials          119         7.8   \n",
       "27  2026-01-10                   Other          120         8.2   \n",
       "28  2026-01-10                 Defense            1         5.3   \n",
       "29  2026-01-10              Healthcare          186         5.8   \n",
       "30  2026-01-10                  Energy          229         5.8   \n",
       "31  2026-01-10      Consumer Defensive           85         5.2   \n",
       "32  2026-01-10             Industrials          194         5.8   \n",
       "33  2026-01-10               Utilities          158         5.8   \n",
       "34  2026-01-10              Technology          294         5.8   \n",
       "35  2026-01-10             Real Estate           24         7.8   \n",
       "36  2026-01-10              Financials          361         5.2   \n",
       "37  2026-01-10       Consumer Cyclical          192         5.8   \n",
       "38  2026-01-10  Communication Services           83         5.8   \n",
       "\n",
       "                                          Reason_Qwen  Score_Llama  \\\n",
       "0   The technology sector has seen a strong moment...          8.2   \n",
       "1   The most recent news indicates a significant m...          7.5   \n",
       "2   The healthcare sector shows mixed signals, wit...          3.8   \n",
       "3   The market sentiment for financials remains la...          4.8   \n",
       "4   The market sentiment for 'Other' has shifted t...          6.8   \n",
       "5   The recent news feed indicates a mixed sentime...          3.8   \n",
       "6   The Industrials sector has shown a recent shif...          4.8   \n",
       "7   The market sentiment for Basic Materials has s...          4.8   \n",
       "8   The Consumer Defensive sector experienced a sh...          4.2   \n",
       "9   The recent news indicates a shift towards a be...          4.2   \n",
       "10  The recent news feed shows a predominantly neg...          3.8   \n",
       "11  The real estate sector experienced a sharp dow...          2.8   \n",
       "12  The energy sector's sentiment has turned beari...          2.8   \n",
       "13  The healthcare sector has shown significant mo...          4.8   \n",
       "14  The most recent news indicates a significant d...          7.5   \n",
       "15  The Consumer Cyclical sector showed strong mom...          7.2   \n",
       "16  The real estate sector experienced a significa...          2.5   \n",
       "17  The market sentiment for Basic Materials has t...          6.2   \n",
       "18  The recent news feed does not heavily favor ei...          4.8   \n",
       "19  The market sentiment for utilities remains lar...          3.8   \n",
       "20  The Industrials sector experienced mixed signa...          4.8   \n",
       "21  The recent news indicates a mixed sentiment to...          2.8   \n",
       "22  The recent news feed does not predominantly fa...          2.5   \n",
       "23  The market sentiment for Communication Service...          4.5   \n",
       "24  The Consumer Defensive sector shows mixed sign...          2.8   \n",
       "25  The recent news primarily highlights financial...          4.2   \n",
       "26  The market for Basic Materials shows a positiv...          6.8   \n",
       "27  The market sentiment for 'Other' has shown a s...          4.8   \n",
       "28  The most recent news indicates a significant m...          7.8   \n",
       "29  The healthcare sector shows mixed signals with...          4.5   \n",
       "30  The market sentiment for Energy has been mixed...          3.8   \n",
       "31  The recent news feed shows mixed signals withi...          5.5   \n",
       "32  The Industrials sector experienced mixed signa...          4.8   \n",
       "33  The market sentiment for Utilities has remaine...          4.5   \n",
       "34  The market sentiment for technology has shown ...          4.2   \n",
       "35  The real estate sector experienced a significa...          2.8   \n",
       "36  The market sentiment for financials remains la...          4.5   \n",
       "37  The Consumer Cyclical sector shows mixed signa...          4.2   \n",
       "38  The recent news feed shows a mixed impact on t...          4.2   \n",
       "\n",
       "                                         Reason_Llama  Score_Gemma  \\\n",
       "0   The recent surge in AI-related news, particula...          7.8   \n",
       "1   The recent news of Lockheed Martin securing a ...          7.8   \n",
       "2   The recent news has been dominated by negative...          7.2   \n",
       "3   The recent news on US oil companies seeking gu...          5.8   \n",
       "4   The market sentiment for 'Other' is bearish du...          4.2   \n",
       "5   The recent news suggests a bearish trend in th...          4.2   \n",
       "6   The Industrials sector is experiencing a beari...          4.2   \n",
       "7   The market sentiment for Basic Materials is be...          4.2   \n",
       "8   The recent news on Conagra, General Mills, and...          4.2   \n",
       "9   The Communication Services sector has taken a ...          4.2   \n",
       "10  The Utilities sector is experiencing a bearish...          4.2   \n",
       "11  The recent news of President Trump's proposal ...          4.8   \n",
       "12  The recent news from the US government's stanc...          4.2   \n",
       "13  The recent news of Merck's acquisition talks w...          8.2   \n",
       "14  The recent news of Lockheed Martin securing a ...          7.8   \n",
       "15  The recent surge in Uniqlo's parent company, F...          4.8   \n",
       "16  The recent news has been dominated by negative...          6.8   \n",
       "17  The recent news of BlueScope's rejection of a ...          6.8   \n",
       "18  The recent news of Strava's IPO filing and Mac...          5.8   \n",
       "19  The Utilities sector is experiencing a bearish...          5.8   \n",
       "20  The Industrials sector is experiencing a beari...          4.2   \n",
       "21  The energy sector is experiencing a bearish se...          5.8   \n",
       "22  The market sentiment for 'Other' has turned be...          6.8   \n",
       "23  The Communication Services sector is experienc...          4.2   \n",
       "24  The Consumer Defensive sector is experiencing ...          4.2   \n",
       "25  The recent news from xAI's significant cash bu...          4.2   \n",
       "26  The Basic Materials sector has shown a positiv...          7.8   \n",
       "27  The market sentiment for 'Other' is currently ...          7.8   \n",
       "28  The recent news of Lockheed Martin securing a ...          7.8   \n",
       "29  The recent news on Edwards Lifesciences' acqui...          7.8   \n",
       "30  The recent news on the Energy sector has turne...          7.8   \n",
       "31  The recent news on Chipotle's potential to bec...          5.8   \n",
       "32  The Industrials sector is experiencing a beari...          5.8   \n",
       "33  The Utilities sector's sentiment has taken a b...          5.8   \n",
       "34  The recent news suggests a bearish sentiment f...          5.8   \n",
       "35  The recent news of Trump's order for Freddie a...          4.2   \n",
       "36  The recent news has been dominated by negative...          4.8   \n",
       "37  The recent news feed suggests a bearish sentim...          4.2   \n",
       "38  The Communication Services sector is experienc...          4.2   \n",
       "\n",
       "                                         Reason_Gemma  Final_Daily_Score  \\\n",
       "0   The Technology sector currently exhibits a str...               8.10   \n",
       "1   The market sentiment for Defense is currently ...               6.75   \n",
       "2   Market sentiment for Healthcare is currently b...               5.43   \n",
       "3   Current market sentiment for Financials appear...               5.30   \n",
       "4   Current market sentiment for 'Other' is decide...               4.58   \n",
       "5   Recent news paints a mixed but ultimately bear...               4.41   \n",
       "6   Recent news indicates a slightly bearish senti...               4.16   \n",
       "7   Recent market sentiment for Basic Materials le...               4.16   \n",
       "8   Recent market sentiment for Consumer Defensive...               4.06   \n",
       "9   The Communication Services sector currently ex...               3.96   \n",
       "10  Current market sentiment for Utilities leans b...               3.83   \n",
       "11  Current market sentiment for real estate is le...               3.70   \n",
       "12  The energy market sentiment is currently leani...               3.40   \n",
       "13  Healthcare sentiment is currently strongly bul...               6.94   \n",
       "14  The market sentiment for Defense is currently ...               6.75   \n",
       "15  The market sentiment for Consumer Cyclicals is...               6.41   \n",
       "16  Despite a slightly negative Time-Weighted Sent...               5.72   \n",
       "17  Current market sentiment for Basic Materials l...               5.48   \n",
       "18  Current market sentiment for Financials appear...               5.30   \n",
       "19  Current market sentiment for Utilities appears...               4.97   \n",
       "20  Despite some positive news regarding General E...               4.94   \n",
       "21  Current market sentiment for Energy is leaning...               4.81   \n",
       "22  Market sentiment for 'Other' is currently bull...               4.77   \n",
       "23  The Communication Services sector currently ex...               4.57   \n",
       "24  Current market sentiment for Consumer Defensiv...               4.11   \n",
       "25  Recent news heavily emphasizes negative develo...               3.96   \n",
       "26  Current market sentiment for Basic Materials i...               7.47   \n",
       "27  Market sentiment for 'Other' is currently stro...               6.95   \n",
       "28  Current market sentiment for the Defense secto...               6.95   \n",
       "29  The healthcare sector currently exhibits a bul...               6.03   \n",
       "30  Market sentiment for Energy is currently bulli...               5.80   \n",
       "31  Current market sentiment for Consumer Defensiv...               5.50   \n",
       "32  The Industrials sector currently presents a mi...               5.47   \n",
       "33  Current market sentiment for Utilities is lean...               5.37   \n",
       "34  Current market sentiment for Technology is lea...               5.27   \n",
       "35  Despite some positive movement in mortgage and...               4.96   \n",
       "36  Current market sentiment for Financials is lea...               4.84   \n",
       "37  Despite some positive momentum from Paranovus,...               4.74   \n",
       "38  Current market sentiment for Communication Ser...               4.74   \n",
       "\n",
       "   Final_Outlook  \n",
       "0        Bullish  \n",
       "1        Bullish  \n",
       "2        Neutral  \n",
       "3        Neutral  \n",
       "4        Neutral  \n",
       "5        Neutral  \n",
       "6        Neutral  \n",
       "7        Neutral  \n",
       "8        Neutral  \n",
       "9        Neutral  \n",
       "10       Neutral  \n",
       "11       Neutral  \n",
       "12       Bearish  \n",
       "13       Bullish  \n",
       "14       Bullish  \n",
       "15       Neutral  \n",
       "16       Neutral  \n",
       "17       Neutral  \n",
       "18       Neutral  \n",
       "19       Neutral  \n",
       "20       Neutral  \n",
       "21       Neutral  \n",
       "22       Neutral  \n",
       "23       Neutral  \n",
       "24       Neutral  \n",
       "25       Neutral  \n",
       "26       Bullish  \n",
       "27       Bullish  \n",
       "28       Bullish  \n",
       "29       Neutral  \n",
       "30       Neutral  \n",
       "31       Neutral  \n",
       "32       Neutral  \n",
       "33       Neutral  \n",
       "34       Neutral  \n",
       "35       Neutral  \n",
       "36       Neutral  \n",
       "37       Neutral  \n",
       "38       Neutral  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv sector_daily_history_7days.csv\n",
    "import pandas as pd\n",
    "df_history = pd.read_csv('csv_checkpoint/sector_daily_history_7days.csv')\n",
    "df_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Sample Project)",
   "language": "python",
   "name": "sample-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
